{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons==0.16.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTFeJGtkIXPz",
        "outputId": "53ec4892-f6e4-45f1-b4eb-7a23ed37a025"
      },
      "id": "YTFeJGtkIXPz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons==0.16.1\n",
            "  Downloading tensorflow_addons-0.16.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard>=2.7 (from tensorflow-addons==0.16.1)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.16.1) (4.6.3)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1 typeguard-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0e1f99d6",
      "metadata": {
        "id": "0e1f99d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fd7722-e749-49f1-e799-cfcc123a2772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0bsjrkuiGouA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a1a8e1-768d-43e1-dad5-95f83b000404"
      },
      "id": "0bsjrkuiGouA",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ee03dbf",
      "metadata": {
        "id": "3ee03dbf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import pywt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "#from keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "#from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c8f3499",
      "metadata": {
        "id": "4c8f3499"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 16\n",
        "num_epochs = 100\n",
        "num_heads= 4\n",
        "projection_dim = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b2bf1f1e",
      "metadata": {
        "id": "b2bf1f1e"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from skimage import color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4624decd",
      "metadata": {
        "id": "4624decd"
      },
      "outputs": [],
      "source": [
        "def get_images(directory):\n",
        "    Images = []\n",
        "    Labels = []  # 0 for BENIGN, 1 for MALIGNANT\n",
        "    label = 0\n",
        "\n",
        "    for labels in os.listdir(directory):\n",
        "        if labels == 'benign':\n",
        "            label = 0\n",
        "        elif labels == 'malignant':\n",
        "            label = 1\n",
        "\n",
        "        for image_file in os.listdir(directory+labels):\n",
        "            image = cv2.imread(directory+labels+r'/'+image_file)\n",
        "            #image = color.rgb2gray(image)\n",
        "            image = cv2.resize(image,(32,32,))\n",
        "            Images.append(image)\n",
        "            Labels.append(label)\n",
        "\n",
        "    return shuffle(Images,Labels,random_state=11)\n",
        "\n",
        "def get_classlabel(class_code):\n",
        "    labels = {0:'benign', 1:'malignant'}\n",
        "\n",
        "    return labels[class_code]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ddf25f73",
      "metadata": {
        "id": "ddf25f73"
      },
      "outputs": [],
      "source": [
        "X,Y = get_images('/content/drive/MyDrive/dataset/Augmented Images 4/')\n",
        "X= np.array(X)\n",
        "Y= np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cccf5e37",
      "metadata": {
        "id": "cccf5e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157d9293-88c8-4228-a9f0-aee2baad5e7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1948, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "612028fc",
      "metadata": {
        "id": "612028fc"
      },
      "outputs": [],
      "source": [
        "image_size = 32\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.RandomCrop(image_size, image_size), layers.RandomFlip(\"horizontal\"),],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "\n",
        "\n",
        "def make_datasets(images, labels, is_train=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(batch_size * 10)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "    return dataset.prefetch(auto)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2c8cedd4",
      "metadata": {
        "id": "2c8cedd4"
      },
      "outputs": [],
      "source": [
        "def fire_module(x, squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
        "    squeeze = layers.Conv2D(squeeze_channels, (1, 1), activation='relu')(x)\n",
        "    expand1x1 = layers.Conv2D(expand1x1_channels, (1, 1), activation='relu')(squeeze)\n",
        "    expand3x3 = layers.Conv2D(expand3x3_channels, (3, 3), padding='same', activation='relu')(squeeze)\n",
        "    return layers.Concatenate()([expand1x1, expand3x3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6bdc6176",
      "metadata": {
        "id": "6bdc6176"
      },
      "outputs": [],
      "source": [
        "\n",
        "def activation_block(x):\n",
        "    x = layers.Activation(\"gelu\")(x)\n",
        "    return layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int):\n",
        "    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "#     x = layers.Flatten()(x)\n",
        "    return activation_block(x)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "    x1 = x\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = fire_module(x, 16, 128, 128)\n",
        "    # x = activation_block(x)\n",
        "    x = layers.Add()([activation_block(x), x1])  # Residual.\n",
        "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=32, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=1\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size)\n",
        "\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size)\n",
        "\n",
        "\n",
        "    # Classification block.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e8a4119a",
      "metadata": {
        "id": "e8a4119a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29eb8a22-3042-456f-9a92-779167fcc91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " rescaling (Rescaling)          (None, 32, 32, 3)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 16, 16, 256)  3328        ['rescaling[0][0]']              \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 16, 16, 256)  0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 16, 16, 256)  1024       ['activation[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 16, 16, 256)  6656       ['batch_normalization[0][0]']    \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 16, 16, 256)  1024       ['activation_1[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 16, 16, 256)  0           ['batch_normalization_1[0][0]',  \n",
            "                                                                  'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 16, 16, 256)  65792       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 16, 16, 16)   4112        ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 16, 16, 128)  2176        ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 16, 16, 128)  18560       ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 16, 16, 256)  0           ['conv2d_3[0][0]',               \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 16, 16, 256)  0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 16, 16, 256)  1024       ['activation_2[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_2[0][0]',  \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 16, 16, 256)  0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 16, 16, 256)  1024       ['activation_3[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_3[0][0]',  \n",
            "                                                                  'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_2[0][0]']                  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 16, 16, 256)  1024       ['activation_4[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_4[0][0]',  \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 16, 16, 256)  65792       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 16, 16, 16)   4112        ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 16, 16, 128)  2176        ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 128)  18560       ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_7[0][0]',               \n",
            "                                                                  'conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 16, 16, 256)  0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 16, 16, 256)  1024       ['activation_5[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_5[0][0]',  \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 16, 16, 256)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 16, 16, 256)  1024       ['activation_6[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_6[0][0]',  \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_5[0][0]']                  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 256)  1024       ['activation_7[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 256)  65792       ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_11[0][0]',              \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 256)  0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 256)  1024       ['activation_8[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_8[0][0]',  \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 256)  0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 256)  1024       ['activation_9[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_8[0][0]']                  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 256)  1024       ['activation_10[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_10[0][0]', \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 256)  65792       ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_15[0][0]',              \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 256)  0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 256)  1024       ['activation_11[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_11[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 256)  0           ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 256)  1024       ['activation_12[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_11[0][0]']                 \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 256)  1024       ['activation_13[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 256)  65792       ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_19[0][0]',              \n",
            "                                                                  'conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 256)  0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 256)  1024       ['activation_14[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 256)  0           ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 256)  1024       ['activation_15[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_15[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_14[0][0]']                 \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 256)  1024       ['activation_16[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_16[0][0]', \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 256)  65792       ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_23[0][0]',              \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 16, 16, 256)  0           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 256)  1024       ['activation_17[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_17[0][0]', \n",
            "                                                                  'add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 16, 16, 256)  0           ['add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 256)  1024       ['activation_18[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_18[0][0]', \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_17[0][0]']                 \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 256)  1024       ['activation_19[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_19[0][0]', \n",
            "                                                                  'add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 256)  65792       ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_27[0][0]',              \n",
            "                                                                  'conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 16, 16, 256)  0           ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 256)  1024       ['activation_20[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_20[0][0]', \n",
            "                                                                  'add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 16, 16, 256)  0           ['add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 16, 16, 256)  1024       ['activation_21[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_21[0][0]', \n",
            "                                                                  'add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 16, 16, 256)  6656       ['add_20[0][0]']                 \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 16, 16, 256)  1024       ['activation_22[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_22[0][0]', \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 16, 16, 256)  65792       ['add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_31[0][0]',              \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 16, 16, 256)  0           ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16, 16, 256)  1024       ['activation_23[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_23[0][0]', \n",
            "                                                                  'add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 16, 16, 256)  0           ['add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 16, 16, 256)  1024       ['activation_24[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 16, 16, 256)  0           ['batch_normalization_24[0][0]', \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 256)         0           ['add_23[0][0]']                 \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            257         ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 807,553\n",
            "Trainable params: 794,753\n",
            "Non-trainable params: 12,800\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model=get_conv_mixer_256_8()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5d16c30d",
      "metadata": {
        "id": "5d16c30d"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\"accuracy\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016fb94b-d82a-4a2a-9d73-c4f0d17c7f30",
        "id": "2308H-M0Ohdt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "78/78 [==============================] - 43s 68ms/step - loss: 0.8813 - accuracy: 0.5578 - val_loss: 0.7282 - val_accuracy: 0.5321\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.7298 - accuracy: 0.6132 - val_loss: 0.7044 - val_accuracy: 0.5321\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 4s 48ms/step - loss: 0.6794 - accuracy: 0.6364 - val_loss: 0.7084 - val_accuracy: 0.5321\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 4s 48ms/step - loss: 0.6361 - accuracy: 0.6541 - val_loss: 0.6967 - val_accuracy: 0.5321\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.5855 - accuracy: 0.6734 - val_loss: 0.7036 - val_accuracy: 0.4519\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 4s 48ms/step - loss: 0.5598 - accuracy: 0.7055 - val_loss: 0.8883 - val_accuracy: 0.5321\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 4s 48ms/step - loss: 0.5156 - accuracy: 0.7384 - val_loss: 0.9298 - val_accuracy: 0.5545\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.4967 - accuracy: 0.7560 - val_loss: 0.8046 - val_accuracy: 0.5994\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.4825 - accuracy: 0.7713 - val_loss: 1.2674 - val_accuracy: 0.5705\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.4714 - accuracy: 0.7833 - val_loss: 1.0109 - val_accuracy: 0.6571\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.4513 - accuracy: 0.7881 - val_loss: 2.3162 - val_accuracy: 0.5385\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.4479 - accuracy: 0.8042 - val_loss: 0.9631 - val_accuracy: 0.6891\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.3947 - accuracy: 0.8226 - val_loss: 2.5969 - val_accuracy: 0.5705\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.3455 - accuracy: 0.8331 - val_loss: 1.7613 - val_accuracy: 0.6090\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.2631 - accuracy: 0.8900 - val_loss: 1.9876 - val_accuracy: 0.6026\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.2459 - accuracy: 0.8925 - val_loss: 1.3359 - val_accuracy: 0.6731\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.2895 - accuracy: 0.8844 - val_loss: 1.4129 - val_accuracy: 0.6442\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.2568 - accuracy: 0.8852 - val_loss: 3.9164 - val_accuracy: 0.5449\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.2179 - accuracy: 0.9173 - val_loss: 1.5070 - val_accuracy: 0.6442\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.1821 - accuracy: 0.9270 - val_loss: 0.9366 - val_accuracy: 0.7051\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1790 - accuracy: 0.9262 - val_loss: 2.6811 - val_accuracy: 0.6218\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1791 - accuracy: 0.9342 - val_loss: 1.4986 - val_accuracy: 0.6571\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.1814 - accuracy: 0.9246 - val_loss: 2.8469 - val_accuracy: 0.5449\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.1253 - accuracy: 0.9575 - val_loss: 1.1888 - val_accuracy: 0.7532\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1130 - accuracy: 0.9607 - val_loss: 1.3755 - val_accuracy: 0.7019\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1089 - accuracy: 0.9575 - val_loss: 1.2620 - val_accuracy: 0.7019\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0881 - accuracy: 0.9631 - val_loss: 5.4094 - val_accuracy: 0.5545\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1130 - accuracy: 0.9639 - val_loss: 1.6595 - val_accuracy: 0.6827\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1709 - accuracy: 0.9358 - val_loss: 1.9023 - val_accuracy: 0.6891\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.1723 - accuracy: 0.9238 - val_loss: 18.8425 - val_accuracy: 0.5481\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1288 - accuracy: 0.9510 - val_loss: 1.2229 - val_accuracy: 0.7340\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1409 - accuracy: 0.9438 - val_loss: 1.0964 - val_accuracy: 0.7019\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.0967 - accuracy: 0.9711 - val_loss: 2.4959 - val_accuracy: 0.5673\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.0504 - accuracy: 0.9872 - val_loss: 1.0878 - val_accuracy: 0.7179\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0303 - accuracy: 0.9968 - val_loss: 0.9346 - val_accuracy: 0.7468\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0151 - accuracy: 0.9992 - val_loss: 1.3328 - val_accuracy: 0.6891\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0162 - accuracy: 0.9968 - val_loss: 2.4028 - val_accuracy: 0.6538\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0102 - accuracy: 0.9984 - val_loss: 0.9027 - val_accuracy: 0.7660\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.8419 - val_accuracy: 0.8013\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8054 - val_accuracy: 0.8173\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8081 - val_accuracy: 0.8205\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8170 - val_accuracy: 0.8109\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8264 - val_accuracy: 0.8045\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8354 - val_accuracy: 0.8045\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 9.0965e-04 - accuracy: 1.0000 - val_loss: 0.8439 - val_accuracy: 0.8109\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 5s 59ms/step - loss: 8.1753e-04 - accuracy: 1.0000 - val_loss: 0.8523 - val_accuracy: 0.8109\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 7.3975e-04 - accuracy: 1.0000 - val_loss: 0.8602 - val_accuracy: 0.8077\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 6.7308e-04 - accuracy: 1.0000 - val_loss: 0.8678 - val_accuracy: 0.8045\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 6.1522e-04 - accuracy: 1.0000 - val_loss: 0.8750 - val_accuracy: 0.8013\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.6441e-04 - accuracy: 1.0000 - val_loss: 0.8820 - val_accuracy: 0.7981\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.1932e-04 - accuracy: 1.0000 - val_loss: 0.8888 - val_accuracy: 0.8013\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 4.7907e-04 - accuracy: 1.0000 - val_loss: 0.8954 - val_accuracy: 0.8013\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 4.4295e-04 - accuracy: 1.0000 - val_loss: 0.9018 - val_accuracy: 0.8013\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 4.1051e-04 - accuracy: 1.0000 - val_loss: 0.9079 - val_accuracy: 0.8013\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 3.8117e-04 - accuracy: 1.0000 - val_loss: 0.9138 - val_accuracy: 0.8013\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.5454e-04 - accuracy: 1.0000 - val_loss: 0.9196 - val_accuracy: 0.8013\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 3.3027e-04 - accuracy: 1.0000 - val_loss: 0.9254 - val_accuracy: 0.8013\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.0805e-04 - accuracy: 1.0000 - val_loss: 0.9310 - val_accuracy: 0.8013\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.8761e-04 - accuracy: 1.0000 - val_loss: 0.9366 - val_accuracy: 0.7981\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 2.6882e-04 - accuracy: 1.0000 - val_loss: 0.9420 - val_accuracy: 0.7981\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.5146e-04 - accuracy: 1.0000 - val_loss: 0.9474 - val_accuracy: 0.7981\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 2.3536e-04 - accuracy: 1.0000 - val_loss: 0.9527 - val_accuracy: 0.7981\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 2.2038e-04 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.7981\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 2.0660e-04 - accuracy: 1.0000 - val_loss: 0.9632 - val_accuracy: 0.7981\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 4s 57ms/step - loss: 1.9402e-04 - accuracy: 1.0000 - val_loss: 0.9683 - val_accuracy: 0.7981\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.8251e-04 - accuracy: 1.0000 - val_loss: 0.9733 - val_accuracy: 0.7981\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.7183e-04 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.7981\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.6191e-04 - accuracy: 1.0000 - val_loss: 0.9833 - val_accuracy: 0.8013\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.5272e-04 - accuracy: 1.0000 - val_loss: 0.9879 - val_accuracy: 0.8013\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.4414e-04 - accuracy: 1.0000 - val_loss: 0.9928 - val_accuracy: 0.8013\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.3614e-04 - accuracy: 1.0000 - val_loss: 0.9976 - val_accuracy: 0.8013\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.2863e-04 - accuracy: 1.0000 - val_loss: 1.0023 - val_accuracy: 0.8013\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.2160e-04 - accuracy: 1.0000 - val_loss: 1.0071 - val_accuracy: 0.8045\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.1500e-04 - accuracy: 1.0000 - val_loss: 1.0119 - val_accuracy: 0.8045\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.0881e-04 - accuracy: 1.0000 - val_loss: 1.0166 - val_accuracy: 0.8045\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.0300e-04 - accuracy: 1.0000 - val_loss: 1.0213 - val_accuracy: 0.8045\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 9.7547e-05 - accuracy: 1.0000 - val_loss: 1.0260 - val_accuracy: 0.8045\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 9.2408e-05 - accuracy: 1.0000 - val_loss: 1.0306 - val_accuracy: 0.8045\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 8.7571e-05 - accuracy: 1.0000 - val_loss: 1.0354 - val_accuracy: 0.8045\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 8.3015e-05 - accuracy: 1.0000 - val_loss: 1.0400 - val_accuracy: 0.8045\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 7.8715e-05 - accuracy: 1.0000 - val_loss: 1.0444 - val_accuracy: 0.8045\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 7.4642e-05 - accuracy: 1.0000 - val_loss: 1.0488 - val_accuracy: 0.8045\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 7.0822e-05 - accuracy: 1.0000 - val_loss: 1.0535 - val_accuracy: 0.8045\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 6.7209e-05 - accuracy: 1.0000 - val_loss: 1.0581 - val_accuracy: 0.8045\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 6.3802e-05 - accuracy: 1.0000 - val_loss: 1.0627 - val_accuracy: 0.8045\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 6.0584e-05 - accuracy: 1.0000 - val_loss: 1.0673 - val_accuracy: 0.8045\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 5.7539e-05 - accuracy: 1.0000 - val_loss: 1.0720 - val_accuracy: 0.8045\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.4644e-05 - accuracy: 1.0000 - val_loss: 1.0767 - val_accuracy: 0.8045\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.1900e-05 - accuracy: 1.0000 - val_loss: 1.0814 - val_accuracy: 0.8045\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 4.9300e-05 - accuracy: 1.0000 - val_loss: 1.0861 - val_accuracy: 0.8077\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.6848e-05 - accuracy: 1.0000 - val_loss: 1.0908 - val_accuracy: 0.8077\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 4.4527e-05 - accuracy: 1.0000 - val_loss: 1.0954 - val_accuracy: 0.8077\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 4.2334e-05 - accuracy: 1.0000 - val_loss: 1.1002 - val_accuracy: 0.8077\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 4.0258e-05 - accuracy: 1.0000 - val_loss: 1.1050 - val_accuracy: 0.8077\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 3.8294e-05 - accuracy: 1.0000 - val_loss: 1.1098 - val_accuracy: 0.8077\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 3.6433e-05 - accuracy: 1.0000 - val_loss: 1.1146 - val_accuracy: 0.8077\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 3.4675e-05 - accuracy: 1.0000 - val_loss: 1.1194 - val_accuracy: 0.8077\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 3.3014e-05 - accuracy: 1.0000 - val_loss: 1.1242 - val_accuracy: 0.8077\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.1440e-05 - accuracy: 1.0000 - val_loss: 1.1290 - val_accuracy: 0.8077\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 2.9951e-05 - accuracy: 1.0000 - val_loss: 1.1338 - val_accuracy: 0.8077\n",
            "13/13 [==============================] - 1s 32ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 33s 61ms/step - loss: 0.9027 - accuracy: 0.5602 - val_loss: 0.6933 - val_accuracy: 0.4968\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.7080 - accuracy: 0.5915 - val_loss: 0.6990 - val_accuracy: 0.4968\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.6445 - accuracy: 0.6509 - val_loss: 0.7300 - val_accuracy: 0.4968\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.6139 - accuracy: 0.6645 - val_loss: 0.6953 - val_accuracy: 0.4679\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.5887 - accuracy: 0.6926 - val_loss: 0.7157 - val_accuracy: 0.4744\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.5328 - accuracy: 0.7416 - val_loss: 0.7532 - val_accuracy: 0.4808\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4916 - accuracy: 0.7713 - val_loss: 1.0888 - val_accuracy: 0.5064\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4696 - accuracy: 0.7809 - val_loss: 1.5527 - val_accuracy: 0.5288\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.4425 - accuracy: 0.7978 - val_loss: 1.0206 - val_accuracy: 0.5769\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.4184 - accuracy: 0.8082 - val_loss: 0.8498 - val_accuracy: 0.6538\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4194 - accuracy: 0.8050 - val_loss: 1.6974 - val_accuracy: 0.5769\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.3772 - accuracy: 0.8291 - val_loss: 3.5804 - val_accuracy: 0.5288\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.3674 - accuracy: 0.8307 - val_loss: 1.5301 - val_accuracy: 0.5801\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.3185 - accuracy: 0.8716 - val_loss: 1.5388 - val_accuracy: 0.5801\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.3010 - accuracy: 0.8772 - val_loss: 7.4797 - val_accuracy: 0.5096\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.2723 - accuracy: 0.8804 - val_loss: 1.4363 - val_accuracy: 0.6186\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.2771 - accuracy: 0.8876 - val_loss: 3.0732 - val_accuracy: 0.5705\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.2124 - accuracy: 0.9125 - val_loss: 2.0663 - val_accuracy: 0.5833\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.2085 - accuracy: 0.9101 - val_loss: 2.2152 - val_accuracy: 0.5929\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1424 - accuracy: 0.9470 - val_loss: 1.3914 - val_accuracy: 0.6923\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.1462 - accuracy: 0.9518 - val_loss: 1.6415 - val_accuracy: 0.6410\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1089 - accuracy: 0.9559 - val_loss: 1.7837 - val_accuracy: 0.6218\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1702 - accuracy: 0.9318 - val_loss: 3.4841 - val_accuracy: 0.5769\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.1380 - accuracy: 0.9494 - val_loss: 1.2377 - val_accuracy: 0.6635\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.1199 - accuracy: 0.9526 - val_loss: 1.6714 - val_accuracy: 0.6859\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0930 - accuracy: 0.9687 - val_loss: 2.9102 - val_accuracy: 0.6058\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1021 - accuracy: 0.9631 - val_loss: 1.9254 - val_accuracy: 0.6603\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0993 - accuracy: 0.9639 - val_loss: 3.2454 - val_accuracy: 0.5609\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0588 - accuracy: 0.9807 - val_loss: 2.1455 - val_accuracy: 0.6218\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0294 - accuracy: 0.9936 - val_loss: 0.9717 - val_accuracy: 0.7340\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 5s 64ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.9469 - val_accuracy: 0.7308\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.9919 - val_accuracy: 0.7404\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.0143 - val_accuracy: 0.7436\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9982 - val_accuracy: 0.7436\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9968 - val_accuracy: 0.7500\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0008 - val_accuracy: 0.7500\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.0041 - val_accuracy: 0.7532\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.7564\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 9.4462e-04 - accuracy: 1.0000 - val_loss: 1.0111 - val_accuracy: 0.7564\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 8.4473e-04 - accuracy: 1.0000 - val_loss: 1.0154 - val_accuracy: 0.7564\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 7.6059e-04 - accuracy: 1.0000 - val_loss: 1.0198 - val_accuracy: 0.7564\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.8848e-04 - accuracy: 1.0000 - val_loss: 1.0242 - val_accuracy: 0.7564\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.2602e-04 - accuracy: 1.0000 - val_loss: 1.0284 - val_accuracy: 0.7532\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 5.7183e-04 - accuracy: 1.0000 - val_loss: 1.0319 - val_accuracy: 0.7500\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 5.2442e-04 - accuracy: 1.0000 - val_loss: 1.0352 - val_accuracy: 0.7500\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.8235e-04 - accuracy: 1.0000 - val_loss: 1.0390 - val_accuracy: 0.7500\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 4.4481e-04 - accuracy: 1.0000 - val_loss: 1.0431 - val_accuracy: 0.7500\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 4.1118e-04 - accuracy: 1.0000 - val_loss: 1.0476 - val_accuracy: 0.7500\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.8093e-04 - accuracy: 1.0000 - val_loss: 1.0520 - val_accuracy: 0.7500\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 3.5360e-04 - accuracy: 1.0000 - val_loss: 1.0562 - val_accuracy: 0.7500\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.2880e-04 - accuracy: 1.0000 - val_loss: 1.0604 - val_accuracy: 0.7468\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.0616e-04 - accuracy: 1.0000 - val_loss: 1.0649 - val_accuracy: 0.7468\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.8549e-04 - accuracy: 1.0000 - val_loss: 1.0693 - val_accuracy: 0.7468\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.6654e-04 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.7500\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 2.4912e-04 - accuracy: 1.0000 - val_loss: 1.0784 - val_accuracy: 0.7500\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.3308e-04 - accuracy: 1.0000 - val_loss: 1.0831 - val_accuracy: 0.7500\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 2.1832e-04 - accuracy: 1.0000 - val_loss: 1.0876 - val_accuracy: 0.7500\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 2.0473e-04 - accuracy: 1.0000 - val_loss: 1.0921 - val_accuracy: 0.7500\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.9215e-04 - accuracy: 1.0000 - val_loss: 1.0965 - val_accuracy: 0.7500\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.8044e-04 - accuracy: 1.0000 - val_loss: 1.1011 - val_accuracy: 0.7500\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.6957e-04 - accuracy: 1.0000 - val_loss: 1.1058 - val_accuracy: 0.7500\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.5950e-04 - accuracy: 1.0000 - val_loss: 1.1105 - val_accuracy: 0.7468\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.5021e-04 - accuracy: 1.0000 - val_loss: 1.1148 - val_accuracy: 0.7468\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.4153e-04 - accuracy: 1.0000 - val_loss: 1.1193 - val_accuracy: 0.7468\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 1.3345e-04 - accuracy: 1.0000 - val_loss: 1.1238 - val_accuracy: 0.7500\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.2589e-04 - accuracy: 1.0000 - val_loss: 1.1288 - val_accuracy: 0.7500\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.1883e-04 - accuracy: 1.0000 - val_loss: 1.1335 - val_accuracy: 0.7500\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 1.1226e-04 - accuracy: 1.0000 - val_loss: 1.1382 - val_accuracy: 0.7500\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.0614e-04 - accuracy: 1.0000 - val_loss: 1.1430 - val_accuracy: 0.7500\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.0039e-04 - accuracy: 1.0000 - val_loss: 1.1479 - val_accuracy: 0.7500\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 9.5014e-05 - accuracy: 1.0000 - val_loss: 1.1526 - val_accuracy: 0.7500\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 8.9970e-05 - accuracy: 1.0000 - val_loss: 1.1575 - val_accuracy: 0.7500\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 8.5235e-05 - accuracy: 1.0000 - val_loss: 1.1624 - val_accuracy: 0.7500\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 8.0793e-05 - accuracy: 1.0000 - val_loss: 1.1671 - val_accuracy: 0.7500\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 5s 64ms/step - loss: 7.6608e-05 - accuracy: 1.0000 - val_loss: 1.1717 - val_accuracy: 0.7500\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 7.2670e-05 - accuracy: 1.0000 - val_loss: 1.1765 - val_accuracy: 0.7500\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.8958e-05 - accuracy: 1.0000 - val_loss: 1.1811 - val_accuracy: 0.7500\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 6.5457e-05 - accuracy: 1.0000 - val_loss: 1.1858 - val_accuracy: 0.7468\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 6.2156e-05 - accuracy: 1.0000 - val_loss: 1.1906 - val_accuracy: 0.7468\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.9038e-05 - accuracy: 1.0000 - val_loss: 1.1955 - val_accuracy: 0.7468\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 5.6097e-05 - accuracy: 1.0000 - val_loss: 1.2003 - val_accuracy: 0.7404\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 5.3314e-05 - accuracy: 1.0000 - val_loss: 1.2053 - val_accuracy: 0.7404\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.0676e-05 - accuracy: 1.0000 - val_loss: 1.2102 - val_accuracy: 0.7404\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 4.8174e-05 - accuracy: 1.0000 - val_loss: 1.2152 - val_accuracy: 0.7404\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.5803e-05 - accuracy: 1.0000 - val_loss: 1.2200 - val_accuracy: 0.7404\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 4.3559e-05 - accuracy: 1.0000 - val_loss: 1.2249 - val_accuracy: 0.7372\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 4.1427e-05 - accuracy: 1.0000 - val_loss: 1.2297 - val_accuracy: 0.7372\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.9397e-05 - accuracy: 1.0000 - val_loss: 1.2347 - val_accuracy: 0.7372\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.7464e-05 - accuracy: 1.0000 - val_loss: 1.2395 - val_accuracy: 0.7404\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 3.5624e-05 - accuracy: 1.0000 - val_loss: 1.2446 - val_accuracy: 0.7372\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 3.3880e-05 - accuracy: 1.0000 - val_loss: 1.2495 - val_accuracy: 0.7372\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.2242e-05 - accuracy: 1.0000 - val_loss: 1.2542 - val_accuracy: 0.7372\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.0700e-05 - accuracy: 1.0000 - val_loss: 1.2591 - val_accuracy: 0.7372\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.9243e-05 - accuracy: 1.0000 - val_loss: 1.2642 - val_accuracy: 0.7404\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.7867e-05 - accuracy: 1.0000 - val_loss: 1.2691 - val_accuracy: 0.7404\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 2.6561e-05 - accuracy: 1.0000 - val_loss: 1.2739 - val_accuracy: 0.7404\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 2.5324e-05 - accuracy: 1.0000 - val_loss: 1.2787 - val_accuracy: 0.7404\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.4149e-05 - accuracy: 1.0000 - val_loss: 1.2834 - val_accuracy: 0.7404\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 2.3035e-05 - accuracy: 1.0000 - val_loss: 1.2882 - val_accuracy: 0.7404\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.1976e-05 - accuracy: 1.0000 - val_loss: 1.2932 - val_accuracy: 0.7404\n",
            "13/13 [==============================] - 1s 23ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 33s 67ms/step - loss: 0.9126 - accuracy: 0.5674 - val_loss: 0.6970 - val_accuracy: 0.5449\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.7487 - accuracy: 0.5939 - val_loss: 0.7463 - val_accuracy: 0.5449\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.6794 - accuracy: 0.6348 - val_loss: 0.6934 - val_accuracy: 0.4968\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.6544 - accuracy: 0.6557 - val_loss: 0.8528 - val_accuracy: 0.4359\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.6257 - accuracy: 0.6597 - val_loss: 0.7717 - val_accuracy: 0.4615\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.5944 - accuracy: 0.6934 - val_loss: 0.8407 - val_accuracy: 0.4359\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.5694 - accuracy: 0.6934 - val_loss: 1.4752 - val_accuracy: 0.4423\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.5440 - accuracy: 0.7287 - val_loss: 1.1862 - val_accuracy: 0.5449\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.5019 - accuracy: 0.7448 - val_loss: 1.4871 - val_accuracy: 0.5737\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4820 - accuracy: 0.7697 - val_loss: 1.8071 - val_accuracy: 0.5288\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 5s 60ms/step - loss: 0.4470 - accuracy: 0.7897 - val_loss: 5.5178 - val_accuracy: 0.4647\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.4256 - accuracy: 0.8034 - val_loss: 5.0974 - val_accuracy: 0.4551\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.3720 - accuracy: 0.8371 - val_loss: 1.4695 - val_accuracy: 0.5449\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.3330 - accuracy: 0.8499 - val_loss: 2.2706 - val_accuracy: 0.5353\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2935 - accuracy: 0.8788 - val_loss: 2.9678 - val_accuracy: 0.5801\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.2605 - accuracy: 0.8917 - val_loss: 2.1536 - val_accuracy: 0.5737\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.2106 - accuracy: 0.9053 - val_loss: 1.7094 - val_accuracy: 0.6026\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.2624 - accuracy: 0.8852 - val_loss: 3.3345 - val_accuracy: 0.5801\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2200 - accuracy: 0.9125 - val_loss: 1.8438 - val_accuracy: 0.5865\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1750 - accuracy: 0.9294 - val_loss: 1.9804 - val_accuracy: 0.6026\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.1679 - accuracy: 0.9334 - val_loss: 7.6937 - val_accuracy: 0.5641\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1667 - accuracy: 0.9238 - val_loss: 5.7332 - val_accuracy: 0.4840\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1260 - accuracy: 0.9478 - val_loss: 4.0054 - val_accuracy: 0.5288\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.1207 - accuracy: 0.9543 - val_loss: 2.7799 - val_accuracy: 0.5641\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.1353 - accuracy: 0.9510 - val_loss: 2.8677 - val_accuracy: 0.5385\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1195 - accuracy: 0.9518 - val_loss: 1.2925 - val_accuracy: 0.6763\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0606 - accuracy: 0.9799 - val_loss: 2.9435 - val_accuracy: 0.5385\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0491 - accuracy: 0.9839 - val_loss: 1.7564 - val_accuracy: 0.6635\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0335 - accuracy: 0.9920 - val_loss: 1.2032 - val_accuracy: 0.6955\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.3152 - val_accuracy: 0.7147\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0203 - val_accuracy: 0.7340\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.0622 - val_accuracy: 0.7532\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.1166 - val_accuracy: 0.7596\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.1497 - val_accuracy: 0.7532\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1579 - val_accuracy: 0.7564\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1655 - val_accuracy: 0.7596\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 9.1970e-04 - accuracy: 1.0000 - val_loss: 1.1742 - val_accuracy: 0.7660\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 8.0888e-04 - accuracy: 1.0000 - val_loss: 1.1841 - val_accuracy: 0.7660\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 7.1871e-04 - accuracy: 1.0000 - val_loss: 1.1948 - val_accuracy: 0.7628\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 6.4369e-04 - accuracy: 1.0000 - val_loss: 1.2051 - val_accuracy: 0.7628\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 5.8022e-04 - accuracy: 1.0000 - val_loss: 1.2145 - val_accuracy: 0.7564\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 5.2587e-04 - accuracy: 1.0000 - val_loss: 1.2238 - val_accuracy: 0.7596\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.7899e-04 - accuracy: 1.0000 - val_loss: 1.2328 - val_accuracy: 0.7564\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.3781e-04 - accuracy: 1.0000 - val_loss: 1.2415 - val_accuracy: 0.7564\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.0162e-04 - accuracy: 1.0000 - val_loss: 1.2497 - val_accuracy: 0.7628\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.6957e-04 - accuracy: 1.0000 - val_loss: 1.2581 - val_accuracy: 0.7596\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.4091e-04 - accuracy: 1.0000 - val_loss: 1.2661 - val_accuracy: 0.7596\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 3.1522e-04 - accuracy: 1.0000 - val_loss: 1.2741 - val_accuracy: 0.7596\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 2.9205e-04 - accuracy: 1.0000 - val_loss: 1.2819 - val_accuracy: 0.7628\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.7109e-04 - accuracy: 1.0000 - val_loss: 1.2897 - val_accuracy: 0.7596\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 2.5209e-04 - accuracy: 1.0000 - val_loss: 1.2972 - val_accuracy: 0.7596\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 2.3476e-04 - accuracy: 1.0000 - val_loss: 1.3043 - val_accuracy: 0.7596\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.1894e-04 - accuracy: 1.0000 - val_loss: 1.3117 - val_accuracy: 0.7628\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.0443e-04 - accuracy: 1.0000 - val_loss: 1.3187 - val_accuracy: 0.7628\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 4s 57ms/step - loss: 1.9106e-04 - accuracy: 1.0000 - val_loss: 1.3256 - val_accuracy: 0.7628\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.7878e-04 - accuracy: 1.0000 - val_loss: 1.3327 - val_accuracy: 0.7628\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.6747e-04 - accuracy: 1.0000 - val_loss: 1.3399 - val_accuracy: 0.7628\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.5707e-04 - accuracy: 1.0000 - val_loss: 1.3469 - val_accuracy: 0.7628\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.4750e-04 - accuracy: 1.0000 - val_loss: 1.3541 - val_accuracy: 0.7628\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.3870e-04 - accuracy: 1.0000 - val_loss: 1.3615 - val_accuracy: 0.7628\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.3057e-04 - accuracy: 1.0000 - val_loss: 1.3689 - val_accuracy: 0.7628\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.2306e-04 - accuracy: 1.0000 - val_loss: 1.3755 - val_accuracy: 0.7628\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.1610e-04 - accuracy: 1.0000 - val_loss: 1.3826 - val_accuracy: 0.7628\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.0964e-04 - accuracy: 1.0000 - val_loss: 1.3893 - val_accuracy: 0.7628\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.0362e-04 - accuracy: 1.0000 - val_loss: 1.3962 - val_accuracy: 0.7628\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 9.7985e-05 - accuracy: 1.0000 - val_loss: 1.4024 - val_accuracy: 0.7628\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 9.2709e-05 - accuracy: 1.0000 - val_loss: 1.4091 - val_accuracy: 0.7628\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 8.7744e-05 - accuracy: 1.0000 - val_loss: 1.4154 - val_accuracy: 0.7628\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 8.3077e-05 - accuracy: 1.0000 - val_loss: 1.4217 - val_accuracy: 0.7628\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 7.8689e-05 - accuracy: 1.0000 - val_loss: 1.4281 - val_accuracy: 0.7660\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 7.4559e-05 - accuracy: 1.0000 - val_loss: 1.4343 - val_accuracy: 0.7628\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 7.0678e-05 - accuracy: 1.0000 - val_loss: 1.4407 - val_accuracy: 0.7628\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 6.7024e-05 - accuracy: 1.0000 - val_loss: 1.4472 - val_accuracy: 0.7628\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.3584e-05 - accuracy: 1.0000 - val_loss: 1.4533 - val_accuracy: 0.7628\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 6.0348e-05 - accuracy: 1.0000 - val_loss: 1.4594 - val_accuracy: 0.7628\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 5.7302e-05 - accuracy: 1.0000 - val_loss: 1.4657 - val_accuracy: 0.7628\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.4438e-05 - accuracy: 1.0000 - val_loss: 1.4718 - val_accuracy: 0.7628\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 5.1744e-05 - accuracy: 1.0000 - val_loss: 1.4785 - val_accuracy: 0.7628\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 4.9201e-05 - accuracy: 1.0000 - val_loss: 1.4847 - val_accuracy: 0.7628\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.6800e-05 - accuracy: 1.0000 - val_loss: 1.4908 - val_accuracy: 0.7628\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.4536e-05 - accuracy: 1.0000 - val_loss: 1.4971 - val_accuracy: 0.7628\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 4.2392e-05 - accuracy: 1.0000 - val_loss: 1.5032 - val_accuracy: 0.7596\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.0364e-05 - accuracy: 1.0000 - val_loss: 1.5091 - val_accuracy: 0.7628\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.8441e-05 - accuracy: 1.0000 - val_loss: 1.5152 - val_accuracy: 0.7628\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 3.6618e-05 - accuracy: 1.0000 - val_loss: 1.5208 - val_accuracy: 0.7628\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.4893e-05 - accuracy: 1.0000 - val_loss: 1.5267 - val_accuracy: 0.7628\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.3256e-05 - accuracy: 1.0000 - val_loss: 1.5324 - val_accuracy: 0.7628\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 3.1702e-05 - accuracy: 1.0000 - val_loss: 1.5383 - val_accuracy: 0.7628\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.0226e-05 - accuracy: 1.0000 - val_loss: 1.5441 - val_accuracy: 0.7596\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.8821e-05 - accuracy: 1.0000 - val_loss: 1.5499 - val_accuracy: 0.7596\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 2.7487e-05 - accuracy: 1.0000 - val_loss: 1.5558 - val_accuracy: 0.7596\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.6217e-05 - accuracy: 1.0000 - val_loss: 1.5612 - val_accuracy: 0.7596\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.5009e-05 - accuracy: 1.0000 - val_loss: 1.5670 - val_accuracy: 0.7596\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 2.3861e-05 - accuracy: 1.0000 - val_loss: 1.5726 - val_accuracy: 0.7596\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 2.2768e-05 - accuracy: 1.0000 - val_loss: 1.5784 - val_accuracy: 0.7596\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 2.1726e-05 - accuracy: 1.0000 - val_loss: 1.5842 - val_accuracy: 0.7596\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 2.0735e-05 - accuracy: 1.0000 - val_loss: 1.5902 - val_accuracy: 0.7564\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 5s 58ms/step - loss: 1.9791e-05 - accuracy: 1.0000 - val_loss: 1.5959 - val_accuracy: 0.7564\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.8894e-05 - accuracy: 1.0000 - val_loss: 1.6020 - val_accuracy: 0.7564\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.8039e-05 - accuracy: 1.0000 - val_loss: 1.6080 - val_accuracy: 0.7564\n",
            "13/13 [==============================] - 2s 29ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 33s 64ms/step - loss: 0.8632 - accuracy: 0.5670 - val_loss: 0.7212 - val_accuracy: 0.4936\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.7604 - accuracy: 0.6223 - val_loss: 0.7576 - val_accuracy: 0.4936\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.6477 - accuracy: 0.6688 - val_loss: 0.7523 - val_accuracy: 0.4936\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.6220 - accuracy: 0.6872 - val_loss: 0.7306 - val_accuracy: 0.4936\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.5812 - accuracy: 0.7081 - val_loss: 0.7533 - val_accuracy: 0.4936\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.5489 - accuracy: 0.7330 - val_loss: 0.9824 - val_accuracy: 0.4936\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.5194 - accuracy: 0.7466 - val_loss: 1.6614 - val_accuracy: 0.4808\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4917 - accuracy: 0.7586 - val_loss: 1.9233 - val_accuracy: 0.5032\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.4644 - accuracy: 0.7827 - val_loss: 6.1214 - val_accuracy: 0.5032\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4532 - accuracy: 0.8059 - val_loss: 3.1243 - val_accuracy: 0.5353\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.4014 - accuracy: 0.8196 - val_loss: 12.4596 - val_accuracy: 0.5032\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.4047 - accuracy: 0.8164 - val_loss: 1.6047 - val_accuracy: 0.5865\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.3577 - accuracy: 0.8452 - val_loss: 2.1490 - val_accuracy: 0.5833\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.2911 - accuracy: 0.8909 - val_loss: 1.0940 - val_accuracy: 0.6859\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.3477 - accuracy: 0.8597 - val_loss: 1.1545 - val_accuracy: 0.6218\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.2424 - accuracy: 0.9110 - val_loss: 3.9039 - val_accuracy: 0.5321\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2579 - accuracy: 0.8998 - val_loss: 1.3803 - val_accuracy: 0.6186\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2700 - accuracy: 0.8869 - val_loss: 1.7239 - val_accuracy: 0.6218\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.2355 - accuracy: 0.9094 - val_loss: 2.1057 - val_accuracy: 0.6154\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.2730 - accuracy: 0.8829 - val_loss: 2.3003 - val_accuracy: 0.5962\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.2181 - accuracy: 0.9142 - val_loss: 1.4666 - val_accuracy: 0.6731\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.1770 - accuracy: 0.9270 - val_loss: 1.3502 - val_accuracy: 0.6506\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.1486 - accuracy: 0.9447 - val_loss: 2.5662 - val_accuracy: 0.6186\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.1160 - accuracy: 0.9615 - val_loss: 3.1221 - val_accuracy: 0.5577\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.1402 - accuracy: 0.9471 - val_loss: 1.1133 - val_accuracy: 0.7019\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 0.0854 - accuracy: 0.9719 - val_loss: 1.1620 - val_accuracy: 0.6699\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0844 - accuracy: 0.9759 - val_loss: 1.1060 - val_accuracy: 0.6827\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0820 - accuracy: 0.9703 - val_loss: 0.9953 - val_accuracy: 0.6795\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 5s 58ms/step - loss: 0.1089 - accuracy: 0.9591 - val_loss: 4.3679 - val_accuracy: 0.5865\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.1335 - accuracy: 0.9503 - val_loss: 2.8587 - val_accuracy: 0.5769\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.1195 - accuracy: 0.9543 - val_loss: 2.0763 - val_accuracy: 0.6538\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 5s 60ms/step - loss: 0.0994 - accuracy: 0.9679 - val_loss: 3.2316 - val_accuracy: 0.5641\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 4s 57ms/step - loss: 0.0503 - accuracy: 0.9840 - val_loss: 1.4951 - val_accuracy: 0.6571\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0262 - accuracy: 0.9944 - val_loss: 2.0633 - val_accuracy: 0.6218\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0223 - accuracy: 0.9936 - val_loss: 1.2859 - val_accuracy: 0.6859\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0149 - accuracy: 0.9968 - val_loss: 1.2659 - val_accuracy: 0.7019\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.0099 - accuracy: 0.9992 - val_loss: 1.1247 - val_accuracy: 0.7436\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 5s 58ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 1.1526 - val_accuracy: 0.7596\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.9824 - val_accuracy: 0.7628\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.7596\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 4s 58ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9861 - val_accuracy: 0.7596\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9805 - val_accuracy: 0.7628\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 8.8059e-04 - accuracy: 1.0000 - val_loss: 0.9830 - val_accuracy: 0.7596\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 7.7991e-04 - accuracy: 1.0000 - val_loss: 0.9887 - val_accuracy: 0.7660\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 7.0115e-04 - accuracy: 1.0000 - val_loss: 0.9951 - val_accuracy: 0.7660\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 6.3636e-04 - accuracy: 1.0000 - val_loss: 1.0015 - val_accuracy: 0.7660\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 5s 58ms/step - loss: 5.8167e-04 - accuracy: 1.0000 - val_loss: 1.0075 - val_accuracy: 0.7692\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 5.3433e-04 - accuracy: 1.0000 - val_loss: 1.0132 - val_accuracy: 0.7692\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 4.9274e-04 - accuracy: 1.0000 - val_loss: 1.0189 - val_accuracy: 0.7692\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 4s 57ms/step - loss: 4.5589e-04 - accuracy: 1.0000 - val_loss: 1.0245 - val_accuracy: 0.7692\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.2296e-04 - accuracy: 1.0000 - val_loss: 1.0301 - val_accuracy: 0.7628\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.9334e-04 - accuracy: 1.0000 - val_loss: 1.0353 - val_accuracy: 0.7660\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 3.6654e-04 - accuracy: 1.0000 - val_loss: 1.0404 - val_accuracy: 0.7660\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 3.4223e-04 - accuracy: 1.0000 - val_loss: 1.0456 - val_accuracy: 0.7660\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.2011e-04 - accuracy: 1.0000 - val_loss: 1.0510 - val_accuracy: 0.7660\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.9988e-04 - accuracy: 1.0000 - val_loss: 1.0561 - val_accuracy: 0.7660\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.8132e-04 - accuracy: 1.0000 - val_loss: 1.0611 - val_accuracy: 0.7660\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.6426e-04 - accuracy: 1.0000 - val_loss: 1.0661 - val_accuracy: 0.7660\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 2.4851e-04 - accuracy: 1.0000 - val_loss: 1.0711 - val_accuracy: 0.7660\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.3396e-04 - accuracy: 1.0000 - val_loss: 1.0761 - val_accuracy: 0.7660\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.2051e-04 - accuracy: 1.0000 - val_loss: 1.0809 - val_accuracy: 0.7660\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.0805e-04 - accuracy: 1.0000 - val_loss: 1.0858 - val_accuracy: 0.7660\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.9647e-04 - accuracy: 1.0000 - val_loss: 1.0908 - val_accuracy: 0.7660\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.8568e-04 - accuracy: 1.0000 - val_loss: 1.0957 - val_accuracy: 0.7660\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.7563e-04 - accuracy: 1.0000 - val_loss: 1.1003 - val_accuracy: 0.7692\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 4s 57ms/step - loss: 1.6624e-04 - accuracy: 1.0000 - val_loss: 1.1049 - val_accuracy: 0.7724\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.5746e-04 - accuracy: 1.0000 - val_loss: 1.1095 - val_accuracy: 0.7628\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.4922e-04 - accuracy: 1.0000 - val_loss: 1.1141 - val_accuracy: 0.7660\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 1.4149e-04 - accuracy: 1.0000 - val_loss: 1.1186 - val_accuracy: 0.7660\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.3420e-04 - accuracy: 1.0000 - val_loss: 1.1231 - val_accuracy: 0.7660\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.2733e-04 - accuracy: 1.0000 - val_loss: 1.1277 - val_accuracy: 0.7660\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.2085e-04 - accuracy: 1.0000 - val_loss: 1.1323 - val_accuracy: 0.7692\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.1474e-04 - accuracy: 1.0000 - val_loss: 1.1372 - val_accuracy: 0.7692\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 1.0899e-04 - accuracy: 1.0000 - val_loss: 1.1421 - val_accuracy: 0.7692\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.0356e-04 - accuracy: 1.0000 - val_loss: 1.1468 - val_accuracy: 0.7692\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 9.8438e-05 - accuracy: 1.0000 - val_loss: 1.1517 - val_accuracy: 0.7692\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 9.3603e-05 - accuracy: 1.0000 - val_loss: 1.1564 - val_accuracy: 0.7660\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 8.9038e-05 - accuracy: 1.0000 - val_loss: 1.1612 - val_accuracy: 0.7628\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 8.4721e-05 - accuracy: 1.0000 - val_loss: 1.1659 - val_accuracy: 0.7660\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 8.0644e-05 - accuracy: 1.0000 - val_loss: 1.1706 - val_accuracy: 0.7660\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 7.6798e-05 - accuracy: 1.0000 - val_loss: 1.1751 - val_accuracy: 0.7660\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 7.3153e-05 - accuracy: 1.0000 - val_loss: 1.1797 - val_accuracy: 0.7660\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.9695e-05 - accuracy: 1.0000 - val_loss: 1.1842 - val_accuracy: 0.7660\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 6.6410e-05 - accuracy: 1.0000 - val_loss: 1.1888 - val_accuracy: 0.7660\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 6.3287e-05 - accuracy: 1.0000 - val_loss: 1.1933 - val_accuracy: 0.7628\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.0328e-05 - accuracy: 1.0000 - val_loss: 1.1978 - val_accuracy: 0.7628\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 5.7527e-05 - accuracy: 1.0000 - val_loss: 1.2023 - val_accuracy: 0.7628\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 5.4865e-05 - accuracy: 1.0000 - val_loss: 1.2069 - val_accuracy: 0.7628\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 5.2314e-05 - accuracy: 1.0000 - val_loss: 1.2117 - val_accuracy: 0.7628\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 4.9880e-05 - accuracy: 1.0000 - val_loss: 1.2164 - val_accuracy: 0.7628\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 4.7589e-05 - accuracy: 1.0000 - val_loss: 1.2209 - val_accuracy: 0.7628\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.5419e-05 - accuracy: 1.0000 - val_loss: 1.2252 - val_accuracy: 0.7628\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.3354e-05 - accuracy: 1.0000 - val_loss: 1.2296 - val_accuracy: 0.7628\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 4.1385e-05 - accuracy: 1.0000 - val_loss: 1.2341 - val_accuracy: 0.7628\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.9507e-05 - accuracy: 1.0000 - val_loss: 1.2386 - val_accuracy: 0.7628\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.7715e-05 - accuracy: 1.0000 - val_loss: 1.2431 - val_accuracy: 0.7628\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 3.6004e-05 - accuracy: 1.0000 - val_loss: 1.2476 - val_accuracy: 0.7628\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.4370e-05 - accuracy: 1.0000 - val_loss: 1.2521 - val_accuracy: 0.7628\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.2808e-05 - accuracy: 1.0000 - val_loss: 1.2566 - val_accuracy: 0.7628\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 3.1318e-05 - accuracy: 1.0000 - val_loss: 1.2612 - val_accuracy: 0.7628\n",
            "13/13 [==============================] - 1s 31ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 33s 71ms/step - loss: 0.8807 - accuracy: 0.5485 - val_loss: 0.7364 - val_accuracy: 0.5096\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.7020 - accuracy: 0.6231 - val_loss: 0.7539 - val_accuracy: 0.5096\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.6508 - accuracy: 0.6383 - val_loss: 0.7657 - val_accuracy: 0.5096\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.6074 - accuracy: 0.6856 - val_loss: 0.7509 - val_accuracy: 0.5096\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.5822 - accuracy: 0.6977 - val_loss: 0.8034 - val_accuracy: 0.5096\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.5413 - accuracy: 0.7281 - val_loss: 0.9829 - val_accuracy: 0.5096\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.5060 - accuracy: 0.7530 - val_loss: 1.0334 - val_accuracy: 0.5096\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 5s 58ms/step - loss: 0.4869 - accuracy: 0.7747 - val_loss: 1.2714 - val_accuracy: 0.5128\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.4830 - accuracy: 0.7650 - val_loss: 0.7989 - val_accuracy: 0.5897\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.4224 - accuracy: 0.7955 - val_loss: 1.1576 - val_accuracy: 0.5994\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.3943 - accuracy: 0.8140 - val_loss: 1.3669 - val_accuracy: 0.5897\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.3958 - accuracy: 0.8164 - val_loss: 3.7498 - val_accuracy: 0.4936\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.3358 - accuracy: 0.8436 - val_loss: 1.5899 - val_accuracy: 0.6026\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.2758 - accuracy: 0.8885 - val_loss: 1.3171 - val_accuracy: 0.6603\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2966 - accuracy: 0.8677 - val_loss: 5.1390 - val_accuracy: 0.5160\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2445 - accuracy: 0.9054 - val_loss: 3.3164 - val_accuracy: 0.5769\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.2614 - accuracy: 0.8877 - val_loss: 2.9005 - val_accuracy: 0.6186\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.2559 - accuracy: 0.8990 - val_loss: 3.8551 - val_accuracy: 0.5994\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2222 - accuracy: 0.9102 - val_loss: 1.6615 - val_accuracy: 0.6763\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.1743 - accuracy: 0.9334 - val_loss: 1.5306 - val_accuracy: 0.6699\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1202 - accuracy: 0.9551 - val_loss: 1.0679 - val_accuracy: 0.7051\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.1204 - accuracy: 0.9519 - val_loss: 3.6223 - val_accuracy: 0.5962\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.1116 - accuracy: 0.9511 - val_loss: 3.8493 - val_accuracy: 0.6026\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.2439 - accuracy: 0.9014 - val_loss: 1.2642 - val_accuracy: 0.6859\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.1470 - accuracy: 0.9366 - val_loss: 1.1860 - val_accuracy: 0.6731\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.1080 - accuracy: 0.9615 - val_loss: 0.8550 - val_accuracy: 0.7019\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0649 - accuracy: 0.9775 - val_loss: 1.5194 - val_accuracy: 0.6538\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0675 - accuracy: 0.9759 - val_loss: 1.1914 - val_accuracy: 0.7212\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0754 - accuracy: 0.9695 - val_loss: 1.8817 - val_accuracy: 0.6667\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0712 - accuracy: 0.9711 - val_loss: 1.6287 - val_accuracy: 0.7051\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0765 - accuracy: 0.9727 - val_loss: 10.8728 - val_accuracy: 0.5321\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 0.0548 - accuracy: 0.9872 - val_loss: 1.8767 - val_accuracy: 0.6667\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0447 - accuracy: 0.9880 - val_loss: 0.8796 - val_accuracy: 0.7308\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0237 - accuracy: 0.9960 - val_loss: 0.8956 - val_accuracy: 0.7756\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 0.0202 - accuracy: 0.9968 - val_loss: 1.5055 - val_accuracy: 0.6827\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 4s 56ms/step - loss: 0.0143 - accuracy: 0.9968 - val_loss: 1.0618 - val_accuracy: 0.7083\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 0.8791 - val_accuracy: 0.7564\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9117 - val_accuracy: 0.7628\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8627 - val_accuracy: 0.7853\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8651 - val_accuracy: 0.7788\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 8.5381e-04 - accuracy: 1.0000 - val_loss: 0.8715 - val_accuracy: 0.7821\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 7.2999e-04 - accuracy: 1.0000 - val_loss: 0.8783 - val_accuracy: 0.7821\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 6.4027e-04 - accuracy: 1.0000 - val_loss: 0.8842 - val_accuracy: 0.7821\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 5.7053e-04 - accuracy: 1.0000 - val_loss: 0.8898 - val_accuracy: 0.7821\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 5.1373e-04 - accuracy: 1.0000 - val_loss: 0.8948 - val_accuracy: 0.7821\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.6632e-04 - accuracy: 1.0000 - val_loss: 0.8996 - val_accuracy: 0.7756\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 4.2585e-04 - accuracy: 1.0000 - val_loss: 0.9042 - val_accuracy: 0.7756\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 3.9102e-04 - accuracy: 1.0000 - val_loss: 0.9091 - val_accuracy: 0.7756\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 3.6064e-04 - accuracy: 1.0000 - val_loss: 0.9134 - val_accuracy: 0.7756\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 3.3367e-04 - accuracy: 1.0000 - val_loss: 0.9175 - val_accuracy: 0.7756\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 5s 58ms/step - loss: 3.0969e-04 - accuracy: 1.0000 - val_loss: 0.9213 - val_accuracy: 0.7756\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.8821e-04 - accuracy: 1.0000 - val_loss: 0.9250 - val_accuracy: 0.7724\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.6888e-04 - accuracy: 1.0000 - val_loss: 0.9287 - val_accuracy: 0.7724\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.5141e-04 - accuracy: 1.0000 - val_loss: 0.9323 - val_accuracy: 0.7724\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.3550e-04 - accuracy: 1.0000 - val_loss: 0.9360 - val_accuracy: 0.7724\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.2101e-04 - accuracy: 1.0000 - val_loss: 0.9396 - val_accuracy: 0.7724\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.0772e-04 - accuracy: 1.0000 - val_loss: 0.9433 - val_accuracy: 0.7724\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 1.9549e-04 - accuracy: 1.0000 - val_loss: 0.9468 - val_accuracy: 0.7692\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.8420e-04 - accuracy: 1.0000 - val_loss: 0.9503 - val_accuracy: 0.7692\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.7375e-04 - accuracy: 1.0000 - val_loss: 0.9537 - val_accuracy: 0.7692\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 1.6403e-04 - accuracy: 1.0000 - val_loss: 0.9571 - val_accuracy: 0.7692\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.5498e-04 - accuracy: 1.0000 - val_loss: 0.9605 - val_accuracy: 0.7692\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.4655e-04 - accuracy: 1.0000 - val_loss: 0.9638 - val_accuracy: 0.7692\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 1.3868e-04 - accuracy: 1.0000 - val_loss: 0.9672 - val_accuracy: 0.7692\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 4s 49ms/step - loss: 1.3133e-04 - accuracy: 1.0000 - val_loss: 0.9706 - val_accuracy: 0.7692\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.2445e-04 - accuracy: 1.0000 - val_loss: 0.9738 - val_accuracy: 0.7692\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.1800e-04 - accuracy: 1.0000 - val_loss: 0.9770 - val_accuracy: 0.7692\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.1193e-04 - accuracy: 1.0000 - val_loss: 0.9803 - val_accuracy: 0.7692\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 1.0623e-04 - accuracy: 1.0000 - val_loss: 0.9835 - val_accuracy: 0.7692\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 1.0088e-04 - accuracy: 1.0000 - val_loss: 0.9866 - val_accuracy: 0.7724\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 9.5832e-05 - accuracy: 1.0000 - val_loss: 0.9899 - val_accuracy: 0.7724\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 9.1076e-05 - accuracy: 1.0000 - val_loss: 0.9930 - val_accuracy: 0.7724\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 8.6580e-05 - accuracy: 1.0000 - val_loss: 0.9962 - val_accuracy: 0.7724\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 8.2328e-05 - accuracy: 1.0000 - val_loss: 0.9994 - val_accuracy: 0.7724\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 7.8303e-05 - accuracy: 1.0000 - val_loss: 1.0026 - val_accuracy: 0.7724\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 7.4492e-05 - accuracy: 1.0000 - val_loss: 1.0057 - val_accuracy: 0.7724\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 7.0882e-05 - accuracy: 1.0000 - val_loss: 1.0089 - val_accuracy: 0.7724\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 6.7460e-05 - accuracy: 1.0000 - val_loss: 1.0122 - val_accuracy: 0.7724\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 6.4226e-05 - accuracy: 1.0000 - val_loss: 1.0153 - val_accuracy: 0.7724\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 6.1166e-05 - accuracy: 1.0000 - val_loss: 1.0184 - val_accuracy: 0.7724\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 5.8268e-05 - accuracy: 1.0000 - val_loss: 1.0214 - val_accuracy: 0.7724\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 5.5522e-05 - accuracy: 1.0000 - val_loss: 1.0244 - val_accuracy: 0.7724\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 4s 57ms/step - loss: 5.2920e-05 - accuracy: 1.0000 - val_loss: 1.0274 - val_accuracy: 0.7724\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 5.0450e-05 - accuracy: 1.0000 - val_loss: 1.0305 - val_accuracy: 0.7724\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.8105e-05 - accuracy: 1.0000 - val_loss: 1.0336 - val_accuracy: 0.7724\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 4.5878e-05 - accuracy: 1.0000 - val_loss: 1.0367 - val_accuracy: 0.7724\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 4.3759e-05 - accuracy: 1.0000 - val_loss: 1.0398 - val_accuracy: 0.7724\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 4.1747e-05 - accuracy: 1.0000 - val_loss: 1.0429 - val_accuracy: 0.7724\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 4s 55ms/step - loss: 3.9832e-05 - accuracy: 1.0000 - val_loss: 1.0461 - val_accuracy: 0.7724\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.8008e-05 - accuracy: 1.0000 - val_loss: 1.0493 - val_accuracy: 0.7756\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.6273e-05 - accuracy: 1.0000 - val_loss: 1.0525 - val_accuracy: 0.7756\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 5s 61ms/step - loss: 3.4621e-05 - accuracy: 1.0000 - val_loss: 1.0558 - val_accuracy: 0.7756\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 5s 61ms/step - loss: 3.3048e-05 - accuracy: 1.0000 - val_loss: 1.0591 - val_accuracy: 0.7756\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 4s 51ms/step - loss: 3.1548e-05 - accuracy: 1.0000 - val_loss: 1.0623 - val_accuracy: 0.7756\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 3.0119e-05 - accuracy: 1.0000 - val_loss: 1.0655 - val_accuracy: 0.7788\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.8758e-05 - accuracy: 1.0000 - val_loss: 1.0687 - val_accuracy: 0.7788\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.7461e-05 - accuracy: 1.0000 - val_loss: 1.0720 - val_accuracy: 0.7788\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.6225e-05 - accuracy: 1.0000 - val_loss: 1.0752 - val_accuracy: 0.7788\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 4s 52ms/step - loss: 2.5047e-05 - accuracy: 1.0000 - val_loss: 1.0785 - val_accuracy: 0.7821\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 2.3923e-05 - accuracy: 1.0000 - val_loss: 1.0818 - val_accuracy: 0.7821\n",
            "13/13 [==============================] - 1s 23ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "k = 5  # Number of folds for k-fold cross-validation\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=11)\n",
        "\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "all_y_pred_probs = []\n",
        "all_loss = []\n",
        "\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(\n",
        "        X_train, Y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=11\n",
        "    )\n",
        "\n",
        "    train_dataset = make_datasets(x_train, y_train)\n",
        "    val_dataset = make_datasets(x_val, y_val)\n",
        "    test_dataset = make_datasets(X_test, Y_test)\n",
        "\n",
        "    model = get_conv_mixer_256_8()\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=num_epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    x_test = X_test  # Define your test data (e.g., X_test)\n",
        "    y_test = Y_test  # Define your test labels (e.g., Y_test)\n",
        "\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    y_pred = np.round(y_pred_probs).flatten()\n",
        "\n",
        "    all_y_true.extend(y_test)\n",
        "    all_y_pred.extend(y_pred)\n",
        "    all_y_pred_probs.extend(y_pred_probs)\n",
        "    all_loss.append(history.history['loss'][-1])\n",
        "\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    precision_scores.append(precision_score(y_test, y_pred))\n",
        "    recall_scores.append(recall_score(y_test, y_pred))"
      ],
      "id": "2308H-M0Ohdt"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "977bfa5e",
      "metadata": {
        "id": "977bfa5e"
      },
      "outputs": [],
      "source": [
        "# Calculate average metrics\n",
        "avg_accuracy = np.mean(accuracy_scores)\n",
        "avg_precision = np.mean(precision_scores)\n",
        "avg_recall = np.mean(recall_scores)\n",
        "avg_loss = np.mean(all_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ad1bad36",
      "metadata": {
        "id": "ad1bad36"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "# Compute average ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(all_y_true, all_y_pred_probs)\n",
        "roc_auc = auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bbf99d4b",
      "metadata": {
        "id": "bbf99d4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# Compute average confusion matrix\n",
        "cm = confusion_matrix(all_y_true, all_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "907ea575",
      "metadata": {
        "id": "907ea575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "a268cdb4-724f-4505-ef80-e31e0cf812fa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot average ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"Average ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Average ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "008ac7c0",
      "metadata": {
        "id": "008ac7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "outputId": "2016b7bf-9af3-4bc3-820d-2f99265d8123"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "# Plot average confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "            xticklabels=[\"benign\", \"malignant\"],\n",
        "            yticklabels=[\"benign\", \"malignant\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Average Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b095fee8",
      "metadata": {
        "id": "b095fee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d77447-c36c-4a8f-c8e9-23c68e9b8ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.7675\n",
            "Average Precision: 0.7951\n",
            "Average Recall: 0.7259\n",
            "Average Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average Loss: {avg_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}