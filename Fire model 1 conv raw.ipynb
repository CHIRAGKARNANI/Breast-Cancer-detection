{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons==0.16.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTFeJGtkIXPz",
        "outputId": "f3266f55-89e3-4f7c-96df-5a597c05944b"
      },
      "id": "YTFeJGtkIXPz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons==0.16.1\n",
            "  Downloading tensorflow_addons-0.16.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard>=2.7 (from tensorflow-addons==0.16.1)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.16.1) (4.6.3)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1 typeguard-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0e1f99d6",
      "metadata": {
        "id": "0e1f99d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e5db0d-0b53-4e8e-afaf-78bf5c444adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0bsjrkuiGouA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3b24c5-80e4-4bbf-cf2d-d2c0fc0724a7"
      },
      "id": "0bsjrkuiGouA",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ee03dbf",
      "metadata": {
        "id": "3ee03dbf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import pywt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "#from keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "#from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c8f3499",
      "metadata": {
        "id": "4c8f3499"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 16\n",
        "num_epochs = 100\n",
        "num_heads= 4\n",
        "projection_dim = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b2bf1f1e",
      "metadata": {
        "id": "b2bf1f1e"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from skimage import color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4624decd",
      "metadata": {
        "id": "4624decd"
      },
      "outputs": [],
      "source": [
        "# def w2d(img, mode='haar', level=1):\n",
        "#     imArray = img\n",
        "#     #Datatype conversions\n",
        "#     #convert to grayscale\n",
        "#     imArray = cv2.cvtColor( imArray,cv2.COLOR_RGB2GRAY )\n",
        "#     #convert to float\n",
        "#     imArray =  np.float32(imArray)\n",
        "#     imArray /= 255;\n",
        "#     # compute coefficients\n",
        "#     coeffs=pywt.wavedec2(imArray, mode, level=level)\n",
        "\n",
        "#     #Process Coefficients\n",
        "#     coeffs_H=list(coeffs)\n",
        "#     coeffs_H[0] *= 0;\n",
        "\n",
        "#     # reconstruction\n",
        "#     imArray_H=pywt.waverec2(coeffs_H, mode);\n",
        "#     imArray_H *= 255;\n",
        "#     imArray_H =  np.uint8(imArray_H)\n",
        "\n",
        "#     return imArray_H\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images(directory):\n",
        "    Images = []\n",
        "    Labels = []  # 0 for BENIGN, 1 for MALIGNANT\n",
        "    label = 0\n",
        "\n",
        "    for labels in os.listdir(directory):\n",
        "        if labels == 'benign':\n",
        "            label = 0\n",
        "        elif labels == 'malignant':\n",
        "            label = 1\n",
        "\n",
        "        for image_file in os.listdir(directory+labels):\n",
        "            image = cv2.imread(directory+labels+r'/'+image_file)\n",
        "            #image = color.rgb2gray(image)\n",
        "            image = cv2.resize(image,(32,32,))\n",
        "            # wv_trans_img = w2d(image, 'db1', 1)\n",
        "            # wv_trans_img_har = cv2.resize(wv_trans_img, (32, 32,))\n",
        "            # combined_img = np.vstack((image.reshape(32*32*3,1), wv_trans_img_har.reshape(32*32,1)))\n",
        "            Images.append(image)\n",
        "            Labels.append(label)\n",
        "\n",
        "    return shuffle(Images,Labels,random_state=11)\n",
        "\n",
        "def get_classlabel(class_code):\n",
        "    labels = {0:'benign', 1:'malignant'}\n",
        "\n",
        "    return labels[class_code]"
      ],
      "metadata": {
        "id": "0Y5I4TTnJYrk"
      },
      "id": "0Y5I4TTnJYrk",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ddf25f73",
      "metadata": {
        "id": "ddf25f73"
      },
      "outputs": [],
      "source": [
        "X,Y = get_images('/content/drive/MyDrive/dataset/Augmented Images 4/')\n",
        "X= np.array(X)\n",
        "Y= np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cccf5e37",
      "metadata": {
        "id": "cccf5e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419ae97b-f08c-4897-d52c-1c4519db664e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1948, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "612028fc",
      "metadata": {
        "id": "612028fc"
      },
      "outputs": [],
      "source": [
        "image_size = 32\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.RandomCrop(image_size, image_size), layers.RandomFlip(\"horizontal\"),],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "\n",
        "\n",
        "def make_datasets(images, labels, is_train=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(batch_size * 10)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset.prefetch(auto)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2c8cedd4",
      "metadata": {
        "id": "2c8cedd4"
      },
      "outputs": [],
      "source": [
        "def fire_module(x, squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
        "    squeeze = layers.Conv2D(squeeze_channels, (1, 1), activation='relu')(x)\n",
        "    expand1x1 = layers.Conv2D(expand1x1_channels, (1, 1), activation='relu')(squeeze)\n",
        "    expand3x3 = layers.Conv2D(expand3x3_channels, (3, 3), padding='same', activation='relu')(squeeze)\n",
        "    return layers.Concatenate()([expand1x1, expand3x3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6bdc6176",
      "metadata": {
        "id": "6bdc6176"
      },
      "outputs": [],
      "source": [
        "\n",
        "def activation_block(x):\n",
        "    x = layers.Activation(\"gelu\")(x)\n",
        "    return layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int):\n",
        "    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "#     x = layers.Flatten()(x)\n",
        "    return activation_block(x)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "    x1 = x\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = fire_module(x, 16, 128, 128)\n",
        "    x = activation_block(x)\n",
        "    # x = layers.Add()([activation_block(x), x1])  # Residual.\n",
        "    # x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=32, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=1\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size)\n",
        "\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size)\n",
        "\n",
        "\n",
        "    # Classification block.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e8a4119a",
      "metadata": {
        "id": "e8a4119a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562d234c-612c-424d-831a-c8dd57651c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " rescaling (Rescaling)          (None, 32, 32, 3)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 16, 16, 256)  3328        ['rescaling[0][0]']              \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 16, 16, 256)  0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 16, 16, 256)  1024       ['activation[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 16, 16, 256)  6656       ['batch_normalization[0][0]']    \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 16, 16, 256)  1024       ['activation_1[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 16, 16, 256)  0           ['batch_normalization_1[0][0]',  \n",
            "                                                                  'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 16, 16, 256)  65792       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 16, 16, 16)   4112        ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 16, 16, 128)  2176        ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 16, 16, 128)  18560       ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 16, 16, 256)  0           ['conv2d_3[0][0]',               \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 16, 16, 256)  0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 16, 16, 256)  1024       ['activation_2[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_2[0][0]']  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 16, 16, 256)  1024       ['activation_3[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_3[0][0]',  \n",
            "                                                                  'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 16, 16, 256)  65792       ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 16, 16, 16)   4112        ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 16, 16, 128)  2176        ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 128)  18560       ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_7[0][0]',               \n",
            "                                                                  'conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 16, 16, 256)  0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 16, 16, 256)  1024       ['activation_4[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_4[0][0]']  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 16, 16, 256)  1024       ['activation_5[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_5[0][0]',  \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 256)  65792       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_11[0][0]',              \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 16, 16, 256)  0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 16, 16, 256)  1024       ['activation_6[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_6[0][0]']  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 256)  1024       ['activation_7[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 256)  65792       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_15[0][0]',              \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 256)  0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 256)  1024       ['activation_8[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_8[0][0]']  \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 256)  0           ['depthwise_conv2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 256)  1024       ['activation_9[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 256)  65792       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_19[0][0]',              \n",
            "                                                                  'conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 256)  0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 256)  1024       ['activation_10[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_10[0][0]'] \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 256)  1024       ['activation_11[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_11[0][0]', \n",
            "                                                                  'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 256)  65792       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_23[0][0]',              \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 256)  0           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 256)  1024       ['activation_12[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_12[0][0]'] \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 256)  1024       ['activation_13[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 256)  65792       ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_27[0][0]',              \n",
            "                                                                  'conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 256)  0           ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 256)  1024       ['activation_14[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 16, 16, 256)  6656       ['batch_normalization_14[0][0]'] \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 256)  0           ['depthwise_conv2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 256)  1024       ['activation_15[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 16, 16, 256)  0           ['batch_normalization_15[0][0]', \n",
            "                                                                  'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 16, 16, 256)  65792       ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 16, 16, 16)   4112        ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 16, 16, 128)  2176        ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 16, 16, 128)  18560       ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_31[0][0]',              \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 256)  0           ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 256)  1024       ['activation_16[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 256)         0           ['batch_normalization_16[0][0]'] \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            257         ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 799,361\n",
            "Trainable params: 790,657\n",
            "Non-trainable params: 8,704\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model=get_conv_mixer_256_8()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5d16c30d",
      "metadata": {
        "id": "5d16c30d"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\"accuracy\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa068d31-86fd-460c-d983-71441f0d50f0",
        "id": "m1YJBJjCJ08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "78/78 [==============================] - 39s 56ms/step - loss: 0.6970 - accuracy: 0.5498 - val_loss: 0.6976 - val_accuracy: 0.4679\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 4s 47ms/step - loss: 0.6827 - accuracy: 0.5465 - val_loss: 0.6925 - val_accuracy: 0.5321\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6756 - accuracy: 0.5770 - val_loss: 0.7068 - val_accuracy: 0.5321\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6624 - accuracy: 0.5955 - val_loss: 0.7253 - val_accuracy: 0.5321\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.6516 - accuracy: 0.6003 - val_loss: 0.6927 - val_accuracy: 0.5224\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6533 - accuracy: 0.6220 - val_loss: 0.8349 - val_accuracy: 0.4872\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.6527 - accuracy: 0.6228 - val_loss: 1.0972 - val_accuracy: 0.4904\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6457 - accuracy: 0.6164 - val_loss: 1.0095 - val_accuracy: 0.5288\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.6700 - accuracy: 0.5883 - val_loss: 1.0128 - val_accuracy: 0.5321\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6593 - accuracy: 0.5987 - val_loss: 1.1589 - val_accuracy: 0.5417\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6472 - accuracy: 0.6340 - val_loss: 1.5408 - val_accuracy: 0.5321\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.6313 - accuracy: 0.6396 - val_loss: 0.8630 - val_accuracy: 0.5513\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.6177 - accuracy: 0.6509 - val_loss: 0.7243 - val_accuracy: 0.5449\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5906 - accuracy: 0.6734 - val_loss: 0.8954 - val_accuracy: 0.5321\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6027 - accuracy: 0.6701 - val_loss: 1.5462 - val_accuracy: 0.5321\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.5970 - accuracy: 0.6766 - val_loss: 1.7783 - val_accuracy: 0.5321\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5866 - accuracy: 0.6950 - val_loss: 1.7553 - val_accuracy: 0.5385\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5519 - accuracy: 0.7223 - val_loss: 3.3618 - val_accuracy: 0.5321\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5428 - accuracy: 0.7319 - val_loss: 2.2842 - val_accuracy: 0.5224\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.5210 - accuracy: 0.7432 - val_loss: 1.0774 - val_accuracy: 0.5705\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.4970 - accuracy: 0.7560 - val_loss: 2.3039 - val_accuracy: 0.5353\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4806 - accuracy: 0.7648 - val_loss: 1.6380 - val_accuracy: 0.5449\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5039 - accuracy: 0.7472 - val_loss: 1.1546 - val_accuracy: 0.5737\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.4466 - accuracy: 0.7889 - val_loss: 1.1959 - val_accuracy: 0.5962\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4057 - accuracy: 0.8066 - val_loss: 2.1495 - val_accuracy: 0.5417\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.3721 - accuracy: 0.8387 - val_loss: 1.8012 - val_accuracy: 0.5801\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.3841 - accuracy: 0.8242 - val_loss: 3.6370 - val_accuracy: 0.5385\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3588 - accuracy: 0.8347 - val_loss: 1.6221 - val_accuracy: 0.6090\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.3304 - accuracy: 0.8539 - val_loss: 1.7101 - val_accuracy: 0.5449\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.3071 - accuracy: 0.8700 - val_loss: 3.2793 - val_accuracy: 0.5417\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2788 - accuracy: 0.8780 - val_loss: 2.9676 - val_accuracy: 0.5833\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.2734 - accuracy: 0.8836 - val_loss: 2.1896 - val_accuracy: 0.5705\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1830 - accuracy: 0.9222 - val_loss: 1.3066 - val_accuracy: 0.6955\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2071 - accuracy: 0.9069 - val_loss: 1.6060 - val_accuracy: 0.6538\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1949 - accuracy: 0.9270 - val_loss: 1.8831 - val_accuracy: 0.6731\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1530 - accuracy: 0.9382 - val_loss: 1.2367 - val_accuracy: 0.7083\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 3s 45ms/step - loss: 0.1306 - accuracy: 0.9446 - val_loss: 2.0783 - val_accuracy: 0.6154\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1404 - accuracy: 0.9502 - val_loss: 1.8353 - val_accuracy: 0.6346\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1522 - accuracy: 0.9398 - val_loss: 1.6291 - val_accuracy: 0.6635\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1158 - accuracy: 0.9510 - val_loss: 1.4808 - val_accuracy: 0.7019\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0836 - accuracy: 0.9671 - val_loss: 1.9012 - val_accuracy: 0.6635\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1163 - accuracy: 0.9494 - val_loss: 2.6979 - val_accuracy: 0.6218\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1215 - accuracy: 0.9567 - val_loss: 1.3874 - val_accuracy: 0.7115\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0535 - accuracy: 0.9783 - val_loss: 1.5815 - val_accuracy: 0.7147\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0594 - accuracy: 0.9767 - val_loss: 1.6775 - val_accuracy: 0.7308\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0670 - accuracy: 0.9719 - val_loss: 1.9234 - val_accuracy: 0.6891\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0585 - accuracy: 0.9807 - val_loss: 1.5683 - val_accuracy: 0.7115\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0463 - accuracy: 0.9823 - val_loss: 2.0894 - val_accuracy: 0.6955\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0822 - accuracy: 0.9631 - val_loss: 2.6974 - val_accuracy: 0.6218\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0916 - accuracy: 0.9687 - val_loss: 2.8162 - val_accuracy: 0.6218\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1357 - accuracy: 0.9486 - val_loss: 1.5011 - val_accuracy: 0.7179\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0468 - accuracy: 0.9848 - val_loss: 1.7383 - val_accuracy: 0.7212\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0370 - accuracy: 0.9864 - val_loss: 1.7744 - val_accuracy: 0.7244\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0339 - accuracy: 0.9864 - val_loss: 1.5175 - val_accuracy: 0.7212\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0381 - accuracy: 0.9856 - val_loss: 1.5728 - val_accuracy: 0.7372\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.0585 - accuracy: 0.9775 - val_loss: 2.2340 - val_accuracy: 0.6603\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0609 - accuracy: 0.9775 - val_loss: 2.0444 - val_accuracy: 0.7083\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0594 - accuracy: 0.9775 - val_loss: 1.6148 - val_accuracy: 0.7179\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0564 - accuracy: 0.9807 - val_loss: 1.7126 - val_accuracy: 0.7115\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0582 - accuracy: 0.9791 - val_loss: 2.1664 - val_accuracy: 0.7051\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0746 - accuracy: 0.9703 - val_loss: 2.0002 - val_accuracy: 0.6923\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0592 - accuracy: 0.9775 - val_loss: 3.2201 - val_accuracy: 0.6026\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0419 - accuracy: 0.9864 - val_loss: 2.4501 - val_accuracy: 0.6923\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0440 - accuracy: 0.9880 - val_loss: 2.2355 - val_accuracy: 0.6506\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0295 - accuracy: 0.9936 - val_loss: 1.8931 - val_accuracy: 0.7340\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 1.7214 - val_accuracy: 0.7468\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0114 - accuracy: 0.9952 - val_loss: 1.7562 - val_accuracy: 0.7564\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0138 - accuracy: 0.9968 - val_loss: 1.7957 - val_accuracy: 0.7724\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 1.9134 - val_accuracy: 0.7500\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 9.9535e-04 - accuracy: 1.0000 - val_loss: 2.1980 - val_accuracy: 0.7372\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 4.0011e-04 - accuracy: 1.0000 - val_loss: 2.0815 - val_accuracy: 0.7532\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 2.8263e-04 - accuracy: 1.0000 - val_loss: 2.0480 - val_accuracy: 0.7532\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.2778e-04 - accuracy: 1.0000 - val_loss: 2.0397 - val_accuracy: 0.7596\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.9172e-04 - accuracy: 1.0000 - val_loss: 2.0419 - val_accuracy: 0.7660\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 1.6557e-04 - accuracy: 1.0000 - val_loss: 2.0490 - val_accuracy: 0.7628\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.4555e-04 - accuracy: 1.0000 - val_loss: 2.0580 - val_accuracy: 0.7628\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.2965e-04 - accuracy: 1.0000 - val_loss: 2.0679 - val_accuracy: 0.7660\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.1662e-04 - accuracy: 1.0000 - val_loss: 2.0781 - val_accuracy: 0.7660\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 1.0573e-04 - accuracy: 1.0000 - val_loss: 2.0881 - val_accuracy: 0.7660\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 9.6466e-05 - accuracy: 1.0000 - val_loss: 2.0979 - val_accuracy: 0.7660\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 8.8435e-05 - accuracy: 1.0000 - val_loss: 2.1077 - val_accuracy: 0.7660\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 8.1441e-05 - accuracy: 1.0000 - val_loss: 2.1176 - val_accuracy: 0.7660\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 7.5266e-05 - accuracy: 1.0000 - val_loss: 2.1272 - val_accuracy: 0.7660\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 6.9753e-05 - accuracy: 1.0000 - val_loss: 2.1365 - val_accuracy: 0.7660\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 6.4821e-05 - accuracy: 1.0000 - val_loss: 2.1457 - val_accuracy: 0.7660\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 6.0383e-05 - accuracy: 1.0000 - val_loss: 2.1549 - val_accuracy: 0.7692\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 5.6385e-05 - accuracy: 1.0000 - val_loss: 2.1640 - val_accuracy: 0.7692\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 5.2758e-05 - accuracy: 1.0000 - val_loss: 2.1730 - val_accuracy: 0.7692\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 4.9453e-05 - accuracy: 1.0000 - val_loss: 2.1820 - val_accuracy: 0.7692\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 4.6417e-05 - accuracy: 1.0000 - val_loss: 2.1909 - val_accuracy: 0.7692\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 4.3627e-05 - accuracy: 1.0000 - val_loss: 2.1997 - val_accuracy: 0.7692\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 4.1050e-05 - accuracy: 1.0000 - val_loss: 2.2084 - val_accuracy: 0.7692\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.8656e-05 - accuracy: 1.0000 - val_loss: 2.2171 - val_accuracy: 0.7692\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 3.6427e-05 - accuracy: 1.0000 - val_loss: 2.2258 - val_accuracy: 0.7692\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 3.4365e-05 - accuracy: 1.0000 - val_loss: 2.2344 - val_accuracy: 0.7692\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.2455e-05 - accuracy: 1.0000 - val_loss: 2.2431 - val_accuracy: 0.7660\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.0684e-05 - accuracy: 1.0000 - val_loss: 2.2516 - val_accuracy: 0.7660\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.9032e-05 - accuracy: 1.0000 - val_loss: 2.2600 - val_accuracy: 0.7660\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.7487e-05 - accuracy: 1.0000 - val_loss: 2.2684 - val_accuracy: 0.7660\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.6038e-05 - accuracy: 1.0000 - val_loss: 2.2768 - val_accuracy: 0.7660\n",
            "13/13 [==============================] - 1s 25ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 28s 49ms/step - loss: 0.7012 - accuracy: 0.5498 - val_loss: 0.6957 - val_accuracy: 0.4968\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.6834 - accuracy: 0.5674 - val_loss: 0.7180 - val_accuracy: 0.5032\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6754 - accuracy: 0.5835 - val_loss: 0.7055 - val_accuracy: 0.5032\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6651 - accuracy: 0.6051 - val_loss: 0.7080 - val_accuracy: 0.5032\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6615 - accuracy: 0.5811 - val_loss: 0.7680 - val_accuracy: 0.5032\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.6429 - accuracy: 0.6324 - val_loss: 1.0737 - val_accuracy: 0.5032\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6498 - accuracy: 0.6212 - val_loss: 0.7051 - val_accuracy: 0.5737\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6570 - accuracy: 0.5971 - val_loss: 0.6763 - val_accuracy: 0.5962\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6247 - accuracy: 0.6404 - val_loss: 1.8769 - val_accuracy: 0.5128\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.6271 - accuracy: 0.6573 - val_loss: 1.2590 - val_accuracy: 0.5609\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5964 - accuracy: 0.6669 - val_loss: 1.4943 - val_accuracy: 0.5609\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6211 - accuracy: 0.6533 - val_loss: 0.8300 - val_accuracy: 0.5256\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.6065 - accuracy: 0.6701 - val_loss: 1.6020 - val_accuracy: 0.5224\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.5712 - accuracy: 0.7079 - val_loss: 0.9472 - val_accuracy: 0.5865\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5313 - accuracy: 0.7247 - val_loss: 1.8500 - val_accuracy: 0.5353\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5352 - accuracy: 0.7319 - val_loss: 4.3654 - val_accuracy: 0.4968\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.5460 - accuracy: 0.7247 - val_loss: 0.9991 - val_accuracy: 0.5994\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4971 - accuracy: 0.7681 - val_loss: 2.6799 - val_accuracy: 0.5192\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4445 - accuracy: 0.7881 - val_loss: 3.4812 - val_accuracy: 0.5321\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.4375 - accuracy: 0.7945 - val_loss: 1.6329 - val_accuracy: 0.5673\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4592 - accuracy: 0.7809 - val_loss: 1.9764 - val_accuracy: 0.5673\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4465 - accuracy: 0.7865 - val_loss: 1.4175 - val_accuracy: 0.5705\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3943 - accuracy: 0.8242 - val_loss: 3.2969 - val_accuracy: 0.5417\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.3821 - accuracy: 0.8379 - val_loss: 1.9750 - val_accuracy: 0.5897\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.3297 - accuracy: 0.8499 - val_loss: 3.9167 - val_accuracy: 0.5449\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3618 - accuracy: 0.8411 - val_loss: 1.0274 - val_accuracy: 0.6442\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.2961 - accuracy: 0.8596 - val_loss: 1.1276 - val_accuracy: 0.6795\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2718 - accuracy: 0.8909 - val_loss: 1.6460 - val_accuracy: 0.6346\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.2596 - accuracy: 0.8917 - val_loss: 1.6108 - val_accuracy: 0.6346\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.2455 - accuracy: 0.8989 - val_loss: 1.0698 - val_accuracy: 0.6699\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.3167 - accuracy: 0.8620 - val_loss: 3.0970 - val_accuracy: 0.5385\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.2010 - accuracy: 0.9262 - val_loss: 1.0352 - val_accuracy: 0.6571\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1713 - accuracy: 0.9326 - val_loss: 1.5888 - val_accuracy: 0.6731\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.1229 - accuracy: 0.9559 - val_loss: 1.4732 - val_accuracy: 0.6891\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1367 - accuracy: 0.9470 - val_loss: 3.0344 - val_accuracy: 0.6026\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.1962 - accuracy: 0.9117 - val_loss: 3.0566 - val_accuracy: 0.5865\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.1279 - accuracy: 0.9518 - val_loss: 3.0201 - val_accuracy: 0.6186\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1537 - accuracy: 0.9358 - val_loss: 2.8817 - val_accuracy: 0.6122\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1059 - accuracy: 0.9615 - val_loss: 1.5062 - val_accuracy: 0.6763\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.1088 - accuracy: 0.9559 - val_loss: 4.0471 - val_accuracy: 0.5705\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1228 - accuracy: 0.9502 - val_loss: 1.0181 - val_accuracy: 0.7468\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1130 - accuracy: 0.9591 - val_loss: 1.5794 - val_accuracy: 0.6923\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.0773 - accuracy: 0.9719 - val_loss: 2.9905 - val_accuracy: 0.6474\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0782 - accuracy: 0.9711 - val_loss: 1.3223 - val_accuracy: 0.6923\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1190 - accuracy: 0.9567 - val_loss: 1.8757 - val_accuracy: 0.6603\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1111 - accuracy: 0.9591 - val_loss: 1.0853 - val_accuracy: 0.7308\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0522 - accuracy: 0.9807 - val_loss: 1.4858 - val_accuracy: 0.7372\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 1.4677 - val_accuracy: 0.7340\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0223 - accuracy: 0.9952 - val_loss: 1.3147 - val_accuracy: 0.7404\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0970 - accuracy: 0.9615 - val_loss: 2.6836 - val_accuracy: 0.6667\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1125 - accuracy: 0.9583 - val_loss: 3.5040 - val_accuracy: 0.6378\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0526 - accuracy: 0.9791 - val_loss: 3.6736 - val_accuracy: 0.5962\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0429 - accuracy: 0.9807 - val_loss: 1.7187 - val_accuracy: 0.7051\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0383 - accuracy: 0.9880 - val_loss: 1.2108 - val_accuracy: 0.7724\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0359 - accuracy: 0.9839 - val_loss: 1.1705 - val_accuracy: 0.7436\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1149 - accuracy: 0.9607 - val_loss: 1.4490 - val_accuracy: 0.6987\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1365 - accuracy: 0.9454 - val_loss: 1.1057 - val_accuracy: 0.7083\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0856 - accuracy: 0.9679 - val_loss: 2.6810 - val_accuracy: 0.6859\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0315 - accuracy: 0.9880 - val_loss: 1.1309 - val_accuracy: 0.7532\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0191 - accuracy: 0.9960 - val_loss: 1.2819 - val_accuracy: 0.7276\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 1.1524 - val_accuracy: 0.7468\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 1.3611 - val_accuracy: 0.7724\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.7129 - val_accuracy: 0.7532\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.4095 - val_accuracy: 0.7596\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 6.9837e-04 - accuracy: 1.0000 - val_loss: 1.3754 - val_accuracy: 0.7564\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.9284e-04 - accuracy: 1.0000 - val_loss: 1.3829 - val_accuracy: 0.7628\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.0227e-04 - accuracy: 1.0000 - val_loss: 1.3966 - val_accuracy: 0.7692\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.5073e-04 - accuracy: 1.0000 - val_loss: 1.4111 - val_accuracy: 0.7660\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 2.1424e-04 - accuracy: 1.0000 - val_loss: 1.4248 - val_accuracy: 0.7660\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.8665e-04 - accuracy: 1.0000 - val_loss: 1.4377 - val_accuracy: 0.7660\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.6493e-04 - accuracy: 1.0000 - val_loss: 1.4498 - val_accuracy: 0.7628\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.4720e-04 - accuracy: 1.0000 - val_loss: 1.4614 - val_accuracy: 0.7628\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 50ms/step - loss: 1.3255e-04 - accuracy: 1.0000 - val_loss: 1.4721 - val_accuracy: 0.7628\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 1.2012e-04 - accuracy: 1.0000 - val_loss: 1.4822 - val_accuracy: 0.7628\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.0944e-04 - accuracy: 1.0000 - val_loss: 1.4920 - val_accuracy: 0.7628\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.0021e-04 - accuracy: 1.0000 - val_loss: 1.5013 - val_accuracy: 0.7596\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 9.2158e-05 - accuracy: 1.0000 - val_loss: 1.5103 - val_accuracy: 0.7596\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 8.5049e-05 - accuracy: 1.0000 - val_loss: 1.5192 - val_accuracy: 0.7596\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 7.8734e-05 - accuracy: 1.0000 - val_loss: 1.5278 - val_accuracy: 0.7596\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 7.3095e-05 - accuracy: 1.0000 - val_loss: 1.5362 - val_accuracy: 0.7596\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 6.8029e-05 - accuracy: 1.0000 - val_loss: 1.5443 - val_accuracy: 0.7596\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 6.3459e-05 - accuracy: 1.0000 - val_loss: 1.5522 - val_accuracy: 0.7596\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 5.9310e-05 - accuracy: 1.0000 - val_loss: 1.5600 - val_accuracy: 0.7596\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.5526e-05 - accuracy: 1.0000 - val_loss: 1.5677 - val_accuracy: 0.7596\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.2061e-05 - accuracy: 1.0000 - val_loss: 1.5753 - val_accuracy: 0.7596\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 4.8878e-05 - accuracy: 1.0000 - val_loss: 1.5828 - val_accuracy: 0.7596\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 4.5945e-05 - accuracy: 1.0000 - val_loss: 1.5902 - val_accuracy: 0.7596\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 4.3242e-05 - accuracy: 1.0000 - val_loss: 1.5976 - val_accuracy: 0.7596\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 4.0738e-05 - accuracy: 1.0000 - val_loss: 1.6048 - val_accuracy: 0.7596\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 3.8417e-05 - accuracy: 1.0000 - val_loss: 1.6120 - val_accuracy: 0.7596\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.6253e-05 - accuracy: 1.0000 - val_loss: 1.6191 - val_accuracy: 0.7596\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.4238e-05 - accuracy: 1.0000 - val_loss: 1.6261 - val_accuracy: 0.7596\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 3.2358e-05 - accuracy: 1.0000 - val_loss: 1.6330 - val_accuracy: 0.7596\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 3.0602e-05 - accuracy: 1.0000 - val_loss: 1.6399 - val_accuracy: 0.7596\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.8959e-05 - accuracy: 1.0000 - val_loss: 1.6467 - val_accuracy: 0.7596\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 2.7419e-05 - accuracy: 1.0000 - val_loss: 1.6535 - val_accuracy: 0.7596\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.5976e-05 - accuracy: 1.0000 - val_loss: 1.6603 - val_accuracy: 0.7596\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 2.4621e-05 - accuracy: 1.0000 - val_loss: 1.6670 - val_accuracy: 0.7596\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.3347e-05 - accuracy: 1.0000 - val_loss: 1.6738 - val_accuracy: 0.7596\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 2.2149e-05 - accuracy: 1.0000 - val_loss: 1.6805 - val_accuracy: 0.7596\n",
            "13/13 [==============================] - 1s 22ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 28s 49ms/step - loss: 0.6901 - accuracy: 0.5602 - val_loss: 0.7018 - val_accuracy: 0.4551\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6860 - accuracy: 0.5835 - val_loss: 0.7181 - val_accuracy: 0.4551\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6754 - accuracy: 0.5706 - val_loss: 0.7197 - val_accuracy: 0.4551\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 4s 47ms/step - loss: 0.6818 - accuracy: 0.5433 - val_loss: 0.7451 - val_accuracy: 0.4551\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.6714 - accuracy: 0.5859 - val_loss: 0.7582 - val_accuracy: 0.4551\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.6669 - accuracy: 0.5851 - val_loss: 0.6887 - val_accuracy: 0.5673\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6506 - accuracy: 0.6204 - val_loss: 0.8870 - val_accuracy: 0.4583\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6527 - accuracy: 0.6124 - val_loss: 0.8073 - val_accuracy: 0.5449\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6334 - accuracy: 0.6340 - val_loss: 0.9098 - val_accuracy: 0.5737\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6205 - accuracy: 0.6669 - val_loss: 1.7580 - val_accuracy: 0.5705\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.6260 - accuracy: 0.6429 - val_loss: 3.8922 - val_accuracy: 0.5353\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.6096 - accuracy: 0.6717 - val_loss: 2.8451 - val_accuracy: 0.5513\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.5993 - accuracy: 0.6717 - val_loss: 4.4640 - val_accuracy: 0.5449\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.5945 - accuracy: 0.6870 - val_loss: 0.9388 - val_accuracy: 0.6026\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5667 - accuracy: 0.7151 - val_loss: 1.2045 - val_accuracy: 0.5801\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.5858 - accuracy: 0.6782 - val_loss: 0.9052 - val_accuracy: 0.5833\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5697 - accuracy: 0.7103 - val_loss: 1.1712 - val_accuracy: 0.5801\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.5516 - accuracy: 0.7119 - val_loss: 1.7463 - val_accuracy: 0.5513\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5295 - accuracy: 0.7432 - val_loss: 3.6103 - val_accuracy: 0.5353\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.5137 - accuracy: 0.7360 - val_loss: 2.0248 - val_accuracy: 0.5385\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.4894 - accuracy: 0.7608 - val_loss: 2.8333 - val_accuracy: 0.5513\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.4833 - accuracy: 0.7681 - val_loss: 3.9195 - val_accuracy: 0.5513\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4683 - accuracy: 0.7761 - val_loss: 1.1149 - val_accuracy: 0.6058\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4366 - accuracy: 0.8026 - val_loss: 1.6871 - val_accuracy: 0.5769\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.3851 - accuracy: 0.8242 - val_loss: 1.4949 - val_accuracy: 0.6410\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4316 - accuracy: 0.8034 - val_loss: 1.3112 - val_accuracy: 0.6090\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3575 - accuracy: 0.8427 - val_loss: 1.8372 - val_accuracy: 0.6314\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.3255 - accuracy: 0.8652 - val_loss: 1.7007 - val_accuracy: 0.5801\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3127 - accuracy: 0.8563 - val_loss: 1.6466 - val_accuracy: 0.5994\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.2435 - accuracy: 0.8965 - val_loss: 1.7284 - val_accuracy: 0.6410\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.2241 - accuracy: 0.9005 - val_loss: 1.6854 - val_accuracy: 0.5801\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.2288 - accuracy: 0.9061 - val_loss: 1.0881 - val_accuracy: 0.6795\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1846 - accuracy: 0.9334 - val_loss: 1.7206 - val_accuracy: 0.6378\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.2088 - accuracy: 0.9125 - val_loss: 1.3662 - val_accuracy: 0.6667\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1684 - accuracy: 0.9230 - val_loss: 1.7752 - val_accuracy: 0.6378\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.1264 - accuracy: 0.9543 - val_loss: 2.4236 - val_accuracy: 0.6186\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1249 - accuracy: 0.9494 - val_loss: 2.3227 - val_accuracy: 0.6731\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0921 - accuracy: 0.9639 - val_loss: 1.9859 - val_accuracy: 0.6186\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.1427 - accuracy: 0.9342 - val_loss: 1.9123 - val_accuracy: 0.6635\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.1223 - accuracy: 0.9551 - val_loss: 4.0178 - val_accuracy: 0.6058\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0990 - accuracy: 0.9583 - val_loss: 1.0172 - val_accuracy: 0.7340\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0812 - accuracy: 0.9695 - val_loss: 1.7440 - val_accuracy: 0.6859\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.0845 - accuracy: 0.9655 - val_loss: 1.2584 - val_accuracy: 0.6538\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0883 - accuracy: 0.9647 - val_loss: 1.9404 - val_accuracy: 0.6635\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0984 - accuracy: 0.9599 - val_loss: 1.6648 - val_accuracy: 0.6603\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.0962 - accuracy: 0.9575 - val_loss: 1.1436 - val_accuracy: 0.7179\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 1.3954 - val_accuracy: 0.7147\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0479 - accuracy: 0.9815 - val_loss: 1.3157 - val_accuracy: 0.7115\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0418 - accuracy: 0.9880 - val_loss: 1.5132 - val_accuracy: 0.7147\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0368 - accuracy: 0.9864 - val_loss: 2.5276 - val_accuracy: 0.6090\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.1266 - accuracy: 0.9559 - val_loss: 1.7385 - val_accuracy: 0.7051\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0567 - accuracy: 0.9751 - val_loss: 1.9852 - val_accuracy: 0.6923\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.1445 - accuracy: 0.9502 - val_loss: 6.5261 - val_accuracy: 0.5577\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1231 - accuracy: 0.9526 - val_loss: 1.7741 - val_accuracy: 0.6859\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0320 - accuracy: 0.9936 - val_loss: 1.5463 - val_accuracy: 0.7212\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 1.3939 - val_accuracy: 0.7404\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 3s 38ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4057 - val_accuracy: 0.7404\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 7.5559e-04 - accuracy: 1.0000 - val_loss: 1.4817 - val_accuracy: 0.7436\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 4.9434e-04 - accuracy: 1.0000 - val_loss: 1.5158 - val_accuracy: 0.7436\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.9011e-04 - accuracy: 1.0000 - val_loss: 1.5405 - val_accuracy: 0.7436\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.2086e-04 - accuracy: 1.0000 - val_loss: 1.5612 - val_accuracy: 0.7404\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 2.7078e-04 - accuracy: 1.0000 - val_loss: 1.5801 - val_accuracy: 0.7404\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.3278e-04 - accuracy: 1.0000 - val_loss: 1.5980 - val_accuracy: 0.7404\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.0286e-04 - accuracy: 1.0000 - val_loss: 1.6150 - val_accuracy: 0.7436\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.7873e-04 - accuracy: 1.0000 - val_loss: 1.6313 - val_accuracy: 0.7404\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 1.5884e-04 - accuracy: 1.0000 - val_loss: 1.6469 - val_accuracy: 0.7404\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.4217e-04 - accuracy: 1.0000 - val_loss: 1.6621 - val_accuracy: 0.7404\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.2799e-04 - accuracy: 1.0000 - val_loss: 1.6767 - val_accuracy: 0.7404\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 1.1582e-04 - accuracy: 1.0000 - val_loss: 1.6908 - val_accuracy: 0.7436\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.0526e-04 - accuracy: 1.0000 - val_loss: 1.7045 - val_accuracy: 0.7436\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 9.6039e-05 - accuracy: 1.0000 - val_loss: 1.7178 - val_accuracy: 0.7436\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 8.7922e-05 - accuracy: 1.0000 - val_loss: 1.7307 - val_accuracy: 0.7436\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 8.0736e-05 - accuracy: 1.0000 - val_loss: 1.7432 - val_accuracy: 0.7436\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 7.4342e-05 - accuracy: 1.0000 - val_loss: 1.7554 - val_accuracy: 0.7436\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 6.8624e-05 - accuracy: 1.0000 - val_loss: 1.7673 - val_accuracy: 0.7436\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 6.3489e-05 - accuracy: 1.0000 - val_loss: 1.7790 - val_accuracy: 0.7404\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 5.8860e-05 - accuracy: 1.0000 - val_loss: 1.7903 - val_accuracy: 0.7404\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 5.4674e-05 - accuracy: 1.0000 - val_loss: 1.8015 - val_accuracy: 0.7404\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 5.0874e-05 - accuracy: 1.0000 - val_loss: 1.8125 - val_accuracy: 0.7404\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 4.7413e-05 - accuracy: 1.0000 - val_loss: 1.8233 - val_accuracy: 0.7404\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 4.4253e-05 - accuracy: 1.0000 - val_loss: 1.8338 - val_accuracy: 0.7404\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 4.1359e-05 - accuracy: 1.0000 - val_loss: 1.8441 - val_accuracy: 0.7404\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 3.8705e-05 - accuracy: 1.0000 - val_loss: 1.8543 - val_accuracy: 0.7404\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 3.6267e-05 - accuracy: 1.0000 - val_loss: 1.8643 - val_accuracy: 0.7404\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.4021e-05 - accuracy: 1.0000 - val_loss: 1.8741 - val_accuracy: 0.7404\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.1950e-05 - accuracy: 1.0000 - val_loss: 1.8838 - val_accuracy: 0.7404\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.0035e-05 - accuracy: 1.0000 - val_loss: 1.8934 - val_accuracy: 0.7404\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 2.8261e-05 - accuracy: 1.0000 - val_loss: 1.9028 - val_accuracy: 0.7404\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.6616e-05 - accuracy: 1.0000 - val_loss: 1.9122 - val_accuracy: 0.7404\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 2.5088e-05 - accuracy: 1.0000 - val_loss: 1.9213 - val_accuracy: 0.7404\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 2.3666e-05 - accuracy: 1.0000 - val_loss: 1.9304 - val_accuracy: 0.7404\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 2.2341e-05 - accuracy: 1.0000 - val_loss: 1.9394 - val_accuracy: 0.7404\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.1106e-05 - accuracy: 1.0000 - val_loss: 1.9483 - val_accuracy: 0.7404\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.9952e-05 - accuracy: 1.0000 - val_loss: 1.9571 - val_accuracy: 0.7404\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.8873e-05 - accuracy: 1.0000 - val_loss: 1.9658 - val_accuracy: 0.7404\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 1.7863e-05 - accuracy: 1.0000 - val_loss: 1.9745 - val_accuracy: 0.7404\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.6916e-05 - accuracy: 1.0000 - val_loss: 1.9830 - val_accuracy: 0.7404\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.6028e-05 - accuracy: 1.0000 - val_loss: 1.9916 - val_accuracy: 0.7436\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 1.5195e-05 - accuracy: 1.0000 - val_loss: 2.0000 - val_accuracy: 0.7436\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 1.4413e-05 - accuracy: 1.0000 - val_loss: 2.0084 - val_accuracy: 0.7436\n",
            "13/13 [==============================] - 1s 19ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 27s 59ms/step - loss: 0.6954 - accuracy: 0.5357 - val_loss: 0.6937 - val_accuracy: 0.4936\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6812 - accuracy: 0.5469 - val_loss: 0.6973 - val_accuracy: 0.5064\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6655 - accuracy: 0.5982 - val_loss: 0.6975 - val_accuracy: 0.5064\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.6511 - accuracy: 0.6151 - val_loss: 0.7533 - val_accuracy: 0.5064\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6482 - accuracy: 0.6135 - val_loss: 0.7936 - val_accuracy: 0.5064\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.6330 - accuracy: 0.6552 - val_loss: 2.9766 - val_accuracy: 0.4679\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.6150 - accuracy: 0.6720 - val_loss: 1.9975 - val_accuracy: 0.4615\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.6225 - accuracy: 0.6568 - val_loss: 2.4522 - val_accuracy: 0.5256\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6023 - accuracy: 0.6528 - val_loss: 0.9234 - val_accuracy: 0.4872\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.6137 - accuracy: 0.6504 - val_loss: 1.2641 - val_accuracy: 0.5032\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.5886 - accuracy: 0.6816 - val_loss: 1.4068 - val_accuracy: 0.5128\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5756 - accuracy: 0.6953 - val_loss: 1.2913 - val_accuracy: 0.5000\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5597 - accuracy: 0.7113 - val_loss: 2.0565 - val_accuracy: 0.5000\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.5475 - accuracy: 0.7129 - val_loss: 1.8436 - val_accuracy: 0.5096\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.5395 - accuracy: 0.7193 - val_loss: 1.8971 - val_accuracy: 0.4968\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5087 - accuracy: 0.7530 - val_loss: 1.7317 - val_accuracy: 0.5000\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.4945 - accuracy: 0.7602 - val_loss: 2.0376 - val_accuracy: 0.4840\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.4600 - accuracy: 0.7851 - val_loss: 2.8421 - val_accuracy: 0.4968\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4485 - accuracy: 0.7955 - val_loss: 1.3912 - val_accuracy: 0.5321\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4387 - accuracy: 0.8067 - val_loss: 2.8339 - val_accuracy: 0.5096\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 4s 46ms/step - loss: 0.4187 - accuracy: 0.8067 - val_loss: 2.2619 - val_accuracy: 0.5064\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3673 - accuracy: 0.8372 - val_loss: 3.3643 - val_accuracy: 0.4968\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.3560 - accuracy: 0.8452 - val_loss: 1.9966 - val_accuracy: 0.5577\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.3464 - accuracy: 0.8532 - val_loss: 2.1170 - val_accuracy: 0.5481\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.3152 - accuracy: 0.8565 - val_loss: 2.1999 - val_accuracy: 0.5769\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3028 - accuracy: 0.8645 - val_loss: 1.8656 - val_accuracy: 0.5994\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3118 - accuracy: 0.8797 - val_loss: 1.9790 - val_accuracy: 0.5449\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.2304 - accuracy: 0.9030 - val_loss: 3.9012 - val_accuracy: 0.5513\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2527 - accuracy: 0.9038 - val_loss: 2.9220 - val_accuracy: 0.5385\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.2319 - accuracy: 0.8990 - val_loss: 2.3310 - val_accuracy: 0.5994\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.2009 - accuracy: 0.9158 - val_loss: 2.0592 - val_accuracy: 0.6090\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1810 - accuracy: 0.9254 - val_loss: 1.6017 - val_accuracy: 0.6090\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.2179 - accuracy: 0.9054 - val_loss: 2.6526 - val_accuracy: 0.5833\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1637 - accuracy: 0.9358 - val_loss: 1.7875 - val_accuracy: 0.6538\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1838 - accuracy: 0.9214 - val_loss: 1.7946 - val_accuracy: 0.6346\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1730 - accuracy: 0.9270 - val_loss: 1.8338 - val_accuracy: 0.6571\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1314 - accuracy: 0.9471 - val_loss: 1.3877 - val_accuracy: 0.6763\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1461 - accuracy: 0.9374 - val_loss: 3.2942 - val_accuracy: 0.6026\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 3s 45ms/step - loss: 0.0797 - accuracy: 0.9671 - val_loss: 2.5681 - val_accuracy: 0.6506\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1199 - accuracy: 0.9575 - val_loss: 2.9229 - val_accuracy: 0.5801\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.1380 - accuracy: 0.9479 - val_loss: 4.4629 - val_accuracy: 0.5833\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.1322 - accuracy: 0.9479 - val_loss: 2.1274 - val_accuracy: 0.6571\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1049 - accuracy: 0.9591 - val_loss: 3.4499 - val_accuracy: 0.5994\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0624 - accuracy: 0.9751 - val_loss: 1.7284 - val_accuracy: 0.7276\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1024 - accuracy: 0.9607 - val_loss: 3.1922 - val_accuracy: 0.6154\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.1089 - accuracy: 0.9559 - val_loss: 2.8092 - val_accuracy: 0.6250\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0753 - accuracy: 0.9703 - val_loss: 2.1081 - val_accuracy: 0.6699\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0606 - accuracy: 0.9767 - val_loss: 1.7690 - val_accuracy: 0.7340\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0788 - accuracy: 0.9759 - val_loss: 1.7697 - val_accuracy: 0.7115\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0962 - accuracy: 0.9631 - val_loss: 3.9884 - val_accuracy: 0.6218\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0684 - accuracy: 0.9695 - val_loss: 2.1735 - val_accuracy: 0.6635\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0792 - accuracy: 0.9727 - val_loss: 1.5623 - val_accuracy: 0.6923\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0696 - accuracy: 0.9743 - val_loss: 2.0163 - val_accuracy: 0.6763\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0298 - accuracy: 0.9872 - val_loss: 1.7585 - val_accuracy: 0.7019\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0299 - accuracy: 0.9888 - val_loss: 2.6117 - val_accuracy: 0.6827\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0654 - accuracy: 0.9759 - val_loss: 2.0927 - val_accuracy: 0.6538\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0908 - accuracy: 0.9655 - val_loss: 2.2597 - val_accuracy: 0.6827\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0858 - accuracy: 0.9679 - val_loss: 2.2444 - val_accuracy: 0.6538\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0728 - accuracy: 0.9687 - val_loss: 2.2726 - val_accuracy: 0.6923\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 1.8829 - val_accuracy: 0.6731\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0224 - accuracy: 0.9928 - val_loss: 1.8886 - val_accuracy: 0.7179\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0191 - accuracy: 0.9952 - val_loss: 2.3129 - val_accuracy: 0.7276\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0108 - accuracy: 0.9984 - val_loss: 2.1054 - val_accuracy: 0.7404\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0265 - accuracy: 0.9880 - val_loss: 2.3636 - val_accuracy: 0.7083\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1157 - accuracy: 0.9591 - val_loss: 4.7632 - val_accuracy: 0.5545\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.0876 - accuracy: 0.9719 - val_loss: 4.1334 - val_accuracy: 0.5609\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0700 - accuracy: 0.9727 - val_loss: 1.7074 - val_accuracy: 0.7083\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0464 - accuracy: 0.9800 - val_loss: 2.2788 - val_accuracy: 0.6731\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 3s 45ms/step - loss: 0.0224 - accuracy: 0.9928 - val_loss: 2.1924 - val_accuracy: 0.6699\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0565 - accuracy: 0.9783 - val_loss: 2.3753 - val_accuracy: 0.6603\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0347 - accuracy: 0.9896 - val_loss: 1.7398 - val_accuracy: 0.7051\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0191 - accuracy: 0.9960 - val_loss: 1.9546 - val_accuracy: 0.7083\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 2.0073 - val_accuracy: 0.7083\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0065 - accuracy: 0.9992 - val_loss: 2.2541 - val_accuracy: 0.7308\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0081 - accuracy: 0.9984 - val_loss: 2.0902 - val_accuracy: 0.7147\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 2.3115 - val_accuracy: 0.7115\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 2.3219 - val_accuracy: 0.7244\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.2772e-04 - accuracy: 1.0000 - val_loss: 2.2587 - val_accuracy: 0.7276\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.3373e-04 - accuracy: 1.0000 - val_loss: 2.2644 - val_accuracy: 0.7212\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 1.7781e-04 - accuracy: 1.0000 - val_loss: 2.2777 - val_accuracy: 0.7276\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 1.4777e-04 - accuracy: 1.0000 - val_loss: 2.2907 - val_accuracy: 0.7308\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.2793e-04 - accuracy: 1.0000 - val_loss: 2.3035 - val_accuracy: 0.7308\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 1.1501e-04 - accuracy: 1.0000 - val_loss: 2.3150 - val_accuracy: 0.7308\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 1.0114e-04 - accuracy: 1.0000 - val_loss: 2.3265 - val_accuracy: 0.7308\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 9.0878e-05 - accuracy: 1.0000 - val_loss: 2.3379 - val_accuracy: 0.7308\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 8.2493e-05 - accuracy: 1.0000 - val_loss: 2.3487 - val_accuracy: 0.7308\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 7.5425e-05 - accuracy: 1.0000 - val_loss: 2.3590 - val_accuracy: 0.7308\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 6.9380e-05 - accuracy: 1.0000 - val_loss: 2.3692 - val_accuracy: 0.7308\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 6.4125e-05 - accuracy: 1.0000 - val_loss: 2.3792 - val_accuracy: 0.7308\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.9495e-05 - accuracy: 1.0000 - val_loss: 2.3890 - val_accuracy: 0.7308\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 5.5375e-05 - accuracy: 1.0000 - val_loss: 2.3986 - val_accuracy: 0.7308\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.1673e-05 - accuracy: 1.0000 - val_loss: 2.4080 - val_accuracy: 0.7308\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 4.8326e-05 - accuracy: 1.0000 - val_loss: 2.4174 - val_accuracy: 0.7308\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 4.5274e-05 - accuracy: 1.0000 - val_loss: 2.4264 - val_accuracy: 0.7308\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 4.2485e-05 - accuracy: 1.0000 - val_loss: 2.4353 - val_accuracy: 0.7308\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 3.9928e-05 - accuracy: 1.0000 - val_loss: 2.4441 - val_accuracy: 0.7308\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 3.7579e-05 - accuracy: 1.0000 - val_loss: 2.4528 - val_accuracy: 0.7340\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 3.5419e-05 - accuracy: 1.0000 - val_loss: 2.4612 - val_accuracy: 0.7340\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 3.3427e-05 - accuracy: 1.0000 - val_loss: 2.4695 - val_accuracy: 0.7340\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 3.1575e-05 - accuracy: 1.0000 - val_loss: 2.4777 - val_accuracy: 0.7340\n",
            "13/13 [==============================] - 1s 28ms/step\n",
            "Epoch 1/100\n",
            "78/78 [==============================] - 28s 53ms/step - loss: 0.7002 - accuracy: 0.5485 - val_loss: 0.6991 - val_accuracy: 0.4904\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6838 - accuracy: 0.5581 - val_loss: 0.7404 - val_accuracy: 0.4904\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6550 - accuracy: 0.6006 - val_loss: 0.7135 - val_accuracy: 0.4904\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.6450 - accuracy: 0.6295 - val_loss: 0.6955 - val_accuracy: 0.4904\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6269 - accuracy: 0.6544 - val_loss: 0.7482 - val_accuracy: 0.4904\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.6198 - accuracy: 0.6455 - val_loss: 0.7688 - val_accuracy: 0.4904\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.5950 - accuracy: 0.6800 - val_loss: 0.9437 - val_accuracy: 0.4936\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.5887 - accuracy: 0.6921 - val_loss: 1.8939 - val_accuracy: 0.5449\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5883 - accuracy: 0.6897 - val_loss: 3.7399 - val_accuracy: 0.5224\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5374 - accuracy: 0.7257 - val_loss: 3.7076 - val_accuracy: 0.5096\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.5390 - accuracy: 0.7265 - val_loss: 3.6575 - val_accuracy: 0.5032\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.5348 - accuracy: 0.7217 - val_loss: 2.7050 - val_accuracy: 0.5064\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.5134 - accuracy: 0.7586 - val_loss: 3.7286 - val_accuracy: 0.5256\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4730 - accuracy: 0.7827 - val_loss: 1.8461 - val_accuracy: 0.5673\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.4330 - accuracy: 0.7995 - val_loss: 2.4242 - val_accuracy: 0.5481\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4419 - accuracy: 0.7891 - val_loss: 2.2052 - val_accuracy: 0.5577\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.4265 - accuracy: 0.7963 - val_loss: 4.1355 - val_accuracy: 0.5353\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.4106 - accuracy: 0.8011 - val_loss: 1.8878 - val_accuracy: 0.5481\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.3981 - accuracy: 0.8059 - val_loss: 5.9305 - val_accuracy: 0.5032\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.3757 - accuracy: 0.8292 - val_loss: 2.0706 - val_accuracy: 0.5609\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.3236 - accuracy: 0.8605 - val_loss: 2.8027 - val_accuracy: 0.5641\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.3425 - accuracy: 0.8532 - val_loss: 1.2914 - val_accuracy: 0.6282\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.3147 - accuracy: 0.8597 - val_loss: 2.2368 - val_accuracy: 0.5962\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2707 - accuracy: 0.8957 - val_loss: 1.1634 - val_accuracy: 0.6186\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.2670 - accuracy: 0.8821 - val_loss: 1.8578 - val_accuracy: 0.6378\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2298 - accuracy: 0.9046 - val_loss: 2.5939 - val_accuracy: 0.5929\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.2313 - accuracy: 0.8982 - val_loss: 3.4881 - val_accuracy: 0.5385\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1885 - accuracy: 0.9310 - val_loss: 2.0002 - val_accuracy: 0.6442\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.1606 - accuracy: 0.9318 - val_loss: 2.5212 - val_accuracy: 0.6058\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1329 - accuracy: 0.9535 - val_loss: 2.7618 - val_accuracy: 0.6154\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1502 - accuracy: 0.9383 - val_loss: 3.2304 - val_accuracy: 0.6218\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1773 - accuracy: 0.9262 - val_loss: 3.4747 - val_accuracy: 0.5609\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 3s 45ms/step - loss: 0.1722 - accuracy: 0.9302 - val_loss: 1.4041 - val_accuracy: 0.6571\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0908 - accuracy: 0.9647 - val_loss: 1.5616 - val_accuracy: 0.6731\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0767 - accuracy: 0.9687 - val_loss: 2.0705 - val_accuracy: 0.6571\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1123 - accuracy: 0.9519 - val_loss: 2.2434 - val_accuracy: 0.6699\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1106 - accuracy: 0.9615 - val_loss: 1.5103 - val_accuracy: 0.7244\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0787 - accuracy: 0.9703 - val_loss: 1.8261 - val_accuracy: 0.6955\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0683 - accuracy: 0.9719 - val_loss: 1.5118 - val_accuracy: 0.7019\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.0986 - accuracy: 0.9615 - val_loss: 2.6475 - val_accuracy: 0.6218\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0969 - accuracy: 0.9615 - val_loss: 2.1182 - val_accuracy: 0.6795\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0571 - accuracy: 0.9783 - val_loss: 1.8602 - val_accuracy: 0.6955\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0496 - accuracy: 0.9775 - val_loss: 1.4798 - val_accuracy: 0.7244\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.0383 - accuracy: 0.9880 - val_loss: 2.1727 - val_accuracy: 0.6827\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0675 - accuracy: 0.9775 - val_loss: 1.8190 - val_accuracy: 0.7051\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.1517 - accuracy: 0.9407 - val_loss: 4.1785 - val_accuracy: 0.5481\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0941 - accuracy: 0.9647 - val_loss: 3.8048 - val_accuracy: 0.6026\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.0460 - accuracy: 0.9872 - val_loss: 1.5813 - val_accuracy: 0.7276\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0347 - accuracy: 0.9896 - val_loss: 1.5800 - val_accuracy: 0.7276\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0390 - accuracy: 0.9896 - val_loss: 1.5252 - val_accuracy: 0.6987\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.0516 - accuracy: 0.9824 - val_loss: 2.2714 - val_accuracy: 0.6506\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0668 - accuracy: 0.9759 - val_loss: 1.7585 - val_accuracy: 0.7019\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 0.0475 - accuracy: 0.9800 - val_loss: 1.7014 - val_accuracy: 0.7083\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0464 - accuracy: 0.9759 - val_loss: 2.3859 - val_accuracy: 0.6571\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 3s 45ms/step - loss: 0.0925 - accuracy: 0.9679 - val_loss: 2.1633 - val_accuracy: 0.6635\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.1025 - accuracy: 0.9567 - val_loss: 2.4749 - val_accuracy: 0.6635\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0559 - accuracy: 0.9800 - val_loss: 2.6931 - val_accuracy: 0.6282\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0284 - accuracy: 0.9896 - val_loss: 1.7478 - val_accuracy: 0.7308\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0234 - accuracy: 0.9904 - val_loss: 1.7180 - val_accuracy: 0.7340\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0135 - accuracy: 0.9928 - val_loss: 1.7818 - val_accuracy: 0.7212\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.0234 - accuracy: 0.9928 - val_loss: 1.8366 - val_accuracy: 0.7115\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.1236 - accuracy: 0.9599 - val_loss: 1.6176 - val_accuracy: 0.7083\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0475 - accuracy: 0.9880 - val_loss: 1.8484 - val_accuracy: 0.6827\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 1.6503 - val_accuracy: 0.6699\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0165 - accuracy: 0.9936 - val_loss: 1.6719 - val_accuracy: 0.6987\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 0.0226 - accuracy: 0.9904 - val_loss: 1.7441 - val_accuracy: 0.7468\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0275 - accuracy: 0.9920 - val_loss: 1.9146 - val_accuracy: 0.7051\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0984 - accuracy: 0.9631 - val_loss: 2.5343 - val_accuracy: 0.6090\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 0.1184 - accuracy: 0.9535 - val_loss: 2.8603 - val_accuracy: 0.6635\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 0.0410 - accuracy: 0.9896 - val_loss: 1.4861 - val_accuracy: 0.7019\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0129 - accuracy: 0.9984 - val_loss: 1.7102 - val_accuracy: 0.7436\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.6178 - val_accuracy: 0.7340\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 4s 45ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.6778 - val_accuracy: 0.7532\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.0445e-04 - accuracy: 1.0000 - val_loss: 1.6615 - val_accuracy: 0.7500\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 3.7853e-04 - accuracy: 1.0000 - val_loss: 1.6675 - val_accuracy: 0.7500\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 3.0547e-04 - accuracy: 1.0000 - val_loss: 1.6807 - val_accuracy: 0.7500\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 2.5564e-04 - accuracy: 1.0000 - val_loss: 1.6957 - val_accuracy: 0.7500\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 2.1904e-04 - accuracy: 1.0000 - val_loss: 1.7112 - val_accuracy: 0.7532\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.9074e-04 - accuracy: 1.0000 - val_loss: 1.7265 - val_accuracy: 0.7564\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 1.6811e-04 - accuracy: 1.0000 - val_loss: 1.7414 - val_accuracy: 0.7564\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 1.4955e-04 - accuracy: 1.0000 - val_loss: 1.7560 - val_accuracy: 0.7596\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 1.3406e-04 - accuracy: 1.0000 - val_loss: 1.7701 - val_accuracy: 0.7596\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 1.2094e-04 - accuracy: 1.0000 - val_loss: 1.7838 - val_accuracy: 0.7596\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 1.0967e-04 - accuracy: 1.0000 - val_loss: 1.7971 - val_accuracy: 0.7596\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 9.9894e-05 - accuracy: 1.0000 - val_loss: 1.8099 - val_accuracy: 0.7596\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 9.1347e-05 - accuracy: 1.0000 - val_loss: 1.8224 - val_accuracy: 0.7596\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 8.3820e-05 - accuracy: 1.0000 - val_loss: 1.8344 - val_accuracy: 0.7596\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 3s 41ms/step - loss: 7.7140e-05 - accuracy: 1.0000 - val_loss: 1.8461 - val_accuracy: 0.7564\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 7.1181e-05 - accuracy: 1.0000 - val_loss: 1.8575 - val_accuracy: 0.7596\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 6.5839e-05 - accuracy: 1.0000 - val_loss: 1.8687 - val_accuracy: 0.7596\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 6.1024e-05 - accuracy: 1.0000 - val_loss: 1.8797 - val_accuracy: 0.7596\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.6667e-05 - accuracy: 1.0000 - val_loss: 1.8904 - val_accuracy: 0.7596\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 5.2713e-05 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 0.7628\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 4.9114e-05 - accuracy: 1.0000 - val_loss: 1.9114 - val_accuracy: 0.7628\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 4.5829e-05 - accuracy: 1.0000 - val_loss: 1.9216 - val_accuracy: 0.7628\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 4.2823e-05 - accuracy: 1.0000 - val_loss: 1.9317 - val_accuracy: 0.7628\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 3s 40ms/step - loss: 4.0069e-05 - accuracy: 1.0000 - val_loss: 1.9416 - val_accuracy: 0.7628\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 3s 42ms/step - loss: 3.7541e-05 - accuracy: 1.0000 - val_loss: 1.9514 - val_accuracy: 0.7628\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 3s 43ms/step - loss: 3.5215e-05 - accuracy: 1.0000 - val_loss: 1.9610 - val_accuracy: 0.7628\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 3s 39ms/step - loss: 3.3070e-05 - accuracy: 1.0000 - val_loss: 1.9705 - val_accuracy: 0.7596\n",
            "13/13 [==============================] - 1s 21ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "k = 5  # Number of folds for k-fold cross-validation\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=11)\n",
        "\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "all_y_pred_probs = []\n",
        "all_loss = []\n",
        "\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(\n",
        "        X_train, Y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=11\n",
        "    )\n",
        "\n",
        "    train_dataset = make_datasets(x_train, y_train)\n",
        "    val_dataset = make_datasets(x_val, y_val)\n",
        "    test_dataset = make_datasets(X_test, Y_test)\n",
        "\n",
        "    model = get_conv_mixer_256_8()\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=num_epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    x_test = X_test  # Define your test data (e.g., X_test)\n",
        "    y_test = Y_test  # Define your test labels (e.g., Y_test)\n",
        "\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    y_pred = np.round(y_pred_probs).flatten()\n",
        "\n",
        "    all_y_true.extend(y_test)\n",
        "    all_y_pred.extend(y_pred)\n",
        "    all_y_pred_probs.extend(y_pred_probs)\n",
        "    all_loss.append(history.history['loss'][-1])\n",
        "\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    precision_scores.append(precision_score(y_test, y_pred))\n",
        "    recall_scores.append(recall_score(y_test, y_pred))"
      ],
      "id": "m1YJBJjCJ08a"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "977bfa5e",
      "metadata": {
        "id": "977bfa5e"
      },
      "outputs": [],
      "source": [
        "# Calculate average metrics\n",
        "avg_accuracy = np.mean(accuracy_scores)\n",
        "avg_precision = np.mean(precision_scores)\n",
        "avg_recall = np.mean(recall_scores)\n",
        "avg_loss = np.mean(all_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ad1bad36",
      "metadata": {
        "id": "ad1bad36"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "# Compute average ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(all_y_true, all_y_pred_probs)\n",
        "roc_auc = auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bbf99d4b",
      "metadata": {
        "id": "bbf99d4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# Compute average confusion matrix\n",
        "cm = confusion_matrix(all_y_true, all_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "907ea575",
      "metadata": {
        "id": "907ea575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "6c19a186-8536-4ae7-f21a-7c669ed7e92d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot average ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"Average ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Average ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "008ac7c0",
      "metadata": {
        "id": "008ac7c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7188996-2a7f-4b6e-b8af-0e63ff3849f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.7628\n",
            "Average Precision: 0.7847\n",
            "Average Recall: 0.7259\n",
            "Average Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average Loss: {avg_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}